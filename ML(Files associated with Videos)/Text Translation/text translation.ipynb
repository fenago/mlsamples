{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.layers import RepeatVector, Dense, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_url = r\"https://assets.datacamp.com/production/repositories/4609/datasets/644e461abb0910edb038e8b2c4ce7071b5aeca12/vocab_fr.txt\"\n",
    "english_url = r\"https://assets.datacamp.com/production/repositories/4609/datasets/3459f954752fb2fce7c0b29e25f067e9784b69fb/vocab_en.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sentences = requests.get(french_url).text\n",
    "english_sentences = requests.get(english_url).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\\nles Ã©tats-unis est g\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_sentences = french_sentences.split(\"\\n\")\n",
    "english_sentences = english_sentences.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137861"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137861"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_len = 20\n",
    "en_vocab = 100\n",
    "fr_len = 25\n",
    "fr_vocab = 125\n",
    "hsize = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = keras.layers.Input(shape = (en_len,en_vocab))\n",
    "encoded_out, encoded_state =  keras.layers.GRU(hsize, return_state=True)(encoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = RepeatVector(fr_len)(encoded_state)\n",
    "decoder_gru_output = keras.layers.GRU(hsize, return_sequences=True)(decoder_input, initial_state=encoded_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_time = TimeDistributed(Dense(fr_vocab, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = dense_time(decoder_gru_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_translation = keras.models.Model(inputs = encoder_input, outputs = pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_translation.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 20, 100)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru (GRU)                       [(None, 48), (None,  21600       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector (RepeatVector)    (None, 25, 48)       0           gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "gru_1 (GRU)                     (None, 25, 48)       14112       repeat_vector[0][0]              \n",
      "                                                                 gru[0][1]                        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 25, 125)      6125        gru_1[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 41,837\n",
      "Trainable params: 41,837\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "machine_translation.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new jersey is sometimes quiet during autumn , and it is snowy in april .'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_token = Tokenizer(num_words=fr_vocab, oov_token=\"UNK\")\n",
    "english_token = Tokenizer(num_words=en_vocab, oov_token=\"UNK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_token.fit_on_texts(french_sentences)\n",
    "english_token.fit_on_texts(english_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[97, 1, 11, 1, 1]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_token.texts_to_sequences([\"I have never done this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_token.index_word[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 5, 4, 2, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def english_to_vector(sentences):\n",
    "    tokenized_sent = english_token.texts_to_sequences(sentences)\n",
    "    preprocessed = pad_sequences(tokenized_sent, padding=\"post\", truncating=\"post\", maxlen=en_len)\n",
    "    \n",
    "    preprocessed = preprocessed[:,::-1]\n",
    "    preprocessed = to_categorical(preprocessed, num_classes=en_vocab)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = english_to_vector([\"I have never done this\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 100)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def french_to_vector(sentences):\n",
    "    tokenized_sent = french_token.texts_to_sequences(sentences)\n",
    "    preprocessed = pad_sequences(tokenized_sent, padding=\"post\", truncating=\"post\", maxlen=fr_len)\n",
    "    \n",
    "    preprocessed = to_categorical(preprocessed, num_classes=fr_vocab)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_X = english_to_vector(english_sentences)\n",
    "french_y = french_to_vector(french_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137861, 20, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7f119842ee48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137.861"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_X.shape[0] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 5s 482us/sample - loss: 4.6269 - acc: 0.3960\n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 3.0708 - acc: 0.5014\n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 2.6140 - acc: 0.5014\n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 2.5069 - acc: 0.5015\n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 2.4170 - acc: 0.5187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f11ad989048>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translation.fit(english_X[:10000], french_y[:10000], epochs = 5,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples\n",
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 2.2819 - acc: 0.5358\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 2.1596 - acc: 0.5522\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 2s 223us/sample - loss: 2.0887 - acc: 0.5606\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 2.0483 - acc: 0.5631\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 2s 232us/sample - loss: 2.0176 - acc: 0.5667\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.9905 - acc: 0.5699\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 1.9631 - acc: 0.5712\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.9292 - acc: 0.5758\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 2s 229us/sample - loss: 1.8848 - acc: 0.5846\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 2s 242us/sample - loss: 1.8345 - acc: 0.5852\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 1.7803 - acc: 0.5847\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 1.7256 - acc: 0.5901\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 3s 323us/sample - loss: 1.6813 - acc: 0.5941\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 1.6451 - acc: 0.5979\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 1.6149 - acc: 0.6049\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 1.5879 - acc: 0.6101\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 2s 223us/sample - loss: 1.5638 - acc: 0.6171\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.5395 - acc: 0.6244\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 1.5173 - acc: 0.6291\n",
      "Epoch 20/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.4935 - acc: 0.6343\n",
      "Epoch 21/50\n",
      "10000/10000 [==============================] - 2s 225us/sample - loss: 1.4712 - acc: 0.6403\n",
      "Epoch 22/50\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 1.4493 - acc: 0.6427\n",
      "Epoch 23/50\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 1.4279 - acc: 0.6458\n",
      "Epoch 24/50\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 1.4050 - acc: 0.6527\n",
      "Epoch 25/50\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.3832 - acc: 0.6575\n",
      "Epoch 26/50\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 1.3648 - acc: 0.6623\n",
      "Epoch 27/50\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.3443 - acc: 0.6657\n",
      "Epoch 28/50\n",
      "10000/10000 [==============================] - 2s 229us/sample - loss: 1.3266 - acc: 0.6694\n",
      "Epoch 29/50\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.3095 - acc: 0.6724\n",
      "Epoch 30/50\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 1.2940 - acc: 0.6750\n",
      "Epoch 31/50\n",
      "10000/10000 [==============================] - 2s 234us/sample - loss: 1.2790 - acc: 0.6784\n",
      "Epoch 32/50\n",
      "10000/10000 [==============================] - 2s 221us/sample - loss: 1.2667 - acc: 0.6809\n",
      "Epoch 33/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.2519 - acc: 0.6859\n",
      "Epoch 34/50\n",
      "10000/10000 [==============================] - 2s 229us/sample - loss: 1.2409 - acc: 0.6870\n",
      "Epoch 35/50\n",
      "10000/10000 [==============================] - 2s 232us/sample - loss: 1.2278 - acc: 0.6906\n",
      "Epoch 36/50\n",
      "10000/10000 [==============================] - 2s 223us/sample - loss: 1.2182 - acc: 0.6917\n",
      "Epoch 37/50\n",
      "10000/10000 [==============================] - 2s 228us/sample - loss: 1.2082 - acc: 0.6927\n",
      "Epoch 38/50\n",
      "10000/10000 [==============================] - 2s 231us/sample - loss: 1.1956 - acc: 0.6951\n",
      "Epoch 39/50\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 1.1867 - acc: 0.6964\n",
      "Epoch 40/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.1756 - acc: 0.6996\n",
      "Epoch 41/50\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 1.1672 - acc: 0.7016\n",
      "Epoch 42/50\n",
      "10000/10000 [==============================] - 2s 219us/sample - loss: 1.1578 - acc: 0.7039\n",
      "Epoch 43/50\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 1.1495 - acc: 0.7057\n",
      "Epoch 44/50\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.1419 - acc: 0.7073\n",
      "Epoch 45/50\n",
      "10000/10000 [==============================] - 2s 227us/sample - loss: 1.1359 - acc: 0.7076\n",
      "Epoch 46/50\n",
      "10000/10000 [==============================] - 2s 235us/sample - loss: 1.1282 - acc: 0.7089\n",
      "Epoch 47/50\n",
      "10000/10000 [==============================] - 2s 230us/sample - loss: 1.1174 - acc: 0.7104\n",
      "Epoch 48/50\n",
      "10000/10000 [==============================] - 2s 224us/sample - loss: 1.1099 - acc: 0.7116\n",
      "Epoch 49/50\n",
      "10000/10000 [==============================] - 2s 232us/sample - loss: 1.1018 - acc: 0.7132\n",
      "Epoch 50/50\n",
      "10000/10000 [==============================] - 2s 226us/sample - loss: 1.0948 - acc: 0.7144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f117a599f28>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "machine_translation.fit(english_X[:10000], french_y[:10000], epochs = 50,batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_text = [english_sentences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new jersey is sometimes quiet during autumn , and it is snowy in april .']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = english_to_vector(required_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = machine_translation.predict(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1.80368329e-06, 3.38521576e-03, 2.80015287e-04, ...,\n",
       "         1.69900304e-05, 8.38451888e-06, 1.01187696e-04],\n",
       "        [7.40098983e-07, 8.81211273e-03, 8.54672026e-03, ...,\n",
       "         7.23950870e-05, 3.95257739e-05, 2.80389999e-04],\n",
       "        [3.48503386e-06, 5.61072771e-03, 7.60605395e-01, ...,\n",
       "         7.47767772e-05, 4.07623447e-05, 7.58126029e-04],\n",
       "        ...,\n",
       "        [9.97195244e-01, 2.34961757e-04, 7.04787744e-05, ...,\n",
       "         1.70168630e-06, 1.39013741e-06, 4.56218373e-07],\n",
       "        [9.97227013e-01, 2.31085811e-04, 6.99774973e-05, ...,\n",
       "         1.68194674e-06, 1.38182929e-06, 4.49790662e-07],\n",
       "        [9.97245073e-01, 2.28846649e-04, 6.96911957e-05, ...,\n",
       "         1.67034807e-06, 1.37698032e-06, 4.46097999e-07]]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 125)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36, 35,  2, 10, 16,  3,  3,  7,  4,  4,  2,  3,  3,  3,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis = -1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "french_token.index_word[36]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new jersey est jamais au en en et il il est en en en'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([french_token.index_word[index] for index in np.argmax(preds, axis = -1)[0] if index != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
