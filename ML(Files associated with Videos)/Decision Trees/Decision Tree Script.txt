Hello everyone, so I hope you're doing great. In this video, we are going to be discussing about decision tree algorithm. So, this is a hands on lab on decision tree algorithm. So, the first thing that I want to go over in this video is to actually discuss what are the various things that we are going to be discussing in this particular hands on video. So, let us get started.

Okay. So, let us first discuss about the aging now the video. So, the first thing that I'm going to share with you is a data set of whether you should go out playing or not, based on some weather conditions. So we are given some weather conditions in the form of categorical variables, and we need to decide, or a build a model of deciding whether we should go out playing or not, whether it is a good idea to go out playing or not. So, this is the data set I, and I would like to share. Now, the reason that I'm sharing this particular data in the beginning of this tutorial is because decision tree algorithm is best understood, if you are able to understand what kind of data set, is it is a suited on. And the way decision tree works is best explained through data set itself. That is why I have picked up this data set, and I have put it in the front end in the beginning of the slide to kind of make sure that this is the first thing that you look at, and then only we discuss a decision tree algorithm, very briefly. Since this is a hands on lab. So we want to spend more time on our practical aspects. Okay. And the third thing is to important questions while constructing the best decision tree so in in machine learning, or in decision tree. More specifically, whenever we are searching for something, then there are in finite different possibilities of that particular thing. Now, here, for example, for our decision tree, we can have in financially different type of decision tree, but which one will be best. It will all depend on these two important questions. Okay. And how are we going to come up with that best decision tree that we have to decide, using using various concepts that we are going to be a briefly touching upon the first of those concepts, is the concept of information gain. Now information gain you can think of metric, or optimization function that we consider before making any split. And if you don't know what the split is in decision tree then I'm going to be touching upon this also in my future slides. Okay. Then we are going to be discussing about impurity measures, and then we will be expressing information gain as the reduction in impurity of a split. Finally, we will be laying out decision tree algorithm, and that decision tree algorithm will be then laid out in the form of pseudocode and using this pseudocode we are going to turn that that pseudocode into an actual decision tree implementation. Now, notice here that I have returned, where all the features are categorical. And the reason is that the decision tree gets a little bit trickier. If you have to implement in the situation where your features are continuous in nature.

It is not impossible, but it gets a little bit trickier to make sure that the split happens at the best point, but whereas if you're all the features are categorical, and that is why I have picked up this particular dataset where all the features are categorical and further the variable that we want to predict is also categorical. So, it is a little bit easier to, kind of, to, kind of, you know, a big decision tree. Okay, and it, it gives you the point in a very neat manner. What is the basic idea behind decision tree algorithm. And finally we will be looking at the class of decision tree classifier, to kind of see how does B fit decision tree classifier in SK learn using frameworks. Okay. And finally, I'm going to share a task with you, and that task will be on a real world data set, to make sure that you can work on your own task, while also understanding how does a particular algorithm work, so I hope you are able to understand and you're excited to start the video. So let us first of all start by looking at the data set. Should I go out playing or not. Now this particular data set as you can see on your screen is is a, you know, what I call is a fully categorical data set so there are no continuous features in this particular data set. So we have got four features. These four features. and we have got one categorical variable to predict, and that categorical variable is whether one should go out playing or not based on these weather conditions. What is the outlook, what type of temperature we are having, we have, we are having hot temperature mild or cold temperature. What kind of humidity we are having. What kind of windy, or whether it is windy out there or not. So it is given in whether it is true or false. Okay. And we are predicting whether I should go out playing or not in the form of yes or no.

Okay. So, this is the data set that we will be working with. And I'm going to exactly discussing about why it is a good idea to start learning about decision tree with categorical features in later slides. So now let us briefly talk about how does decision tree algorithm work. So decision tree algorithm is one of the most simplest algorithm. If you talk about interpretability. The reason is that the way decision tree works is by asking a bunch of questions about your data set, or rather I should say for sake of simplicity. It asked a bunch of binary questions. And what do we mean by binary questions. With the help of binary equations. It splits the data set into two types, two parts that other. Okay. And with those two parts, it again recursively repeat the same step. And what does that mean that means with each part, it asks, those binary questions, again, but those binary, the binary questions that it asked in the first step will not be seen as the equation as the as the equations which it is going to ask in the step two. So it starts by this, we have a full data set, and it asks a binary question, and based on those binary question, it will split the data set into two parts. And with those two parts, it is going to again ask some other binary questions. Okay, on features. It is again going to split the data set into two parts, and in this way it is, it keeps on splitting the data set into two parts. Until and unless we have got the sufficient.

I would say sufficient criteria or sufficient accuracy. Okay, so we need to also define a way to stop this splitting. Okay. So, I hope you are able to understand, so it just keeps on splitting this data set. And that is why it is known as decision tree because at each step it is making a decision of. Where should. Where should a particular example go, for example, I have a new example. Okay, I have a new, new role to predict whether I should go playing or not for that particular group. Now, for that particular row, it will just keep on asking a new, new duck those equations that we have learned during training process, and it will predict whether we should go out playing or not. Okay. So, that is why it is called one of the most interpretable algorithms out there. Okay. Now, if I want to understand how does decision tree works. I would like to first of all understand these two important questions. Now what are these equations. So, now, remember, we discussed that it is asking binary questions.

And these binary equations are related to features. So, it is essentially asking those binary equations on features, know what kind of binary questions, it can ask. Now, consider this particular data set. the first question that this particular decision tree might ask is whether the weather the wind is there, whether wind is true or false. So notice here, if when if there is a wind, there is a wind, and, and, and another situation is where there is an absence of wind. Now, in those two situations we will have two different data sets. Right. So, first it is asking. First, it is having full data set.

Okay, this full data set. In the root node. So, this is a node root node where decision tree starts splitting. Now, it asks, some questions related to a wind, and it splits the data into two parts. The first part will be whether there is a wind, whether there is an absence of wind. Now, with these two sub data sets. We will not be having these full data sets, obviously the first part will constitute of a data set where, when, windy will be true. And second part where windy will be false. And with these two data sets, it is again going to ask some questions. And how are we going to come up with these, these questions.

Okay, so these questions, we are going to come up with.

Based on these two important points. Okay. So, at each split, we need to understand what is the best feature, like, just now, I have just said that. First I'm going to split on wind, but how do we come up with this particular criteria, which makes our decision because I could have just a split on humidity or I could have just a split on temperature or outlook, but what is the best among all these four. So, for that we need. We need to understand these two important questions. The first question is, if I have n features. Now, I'm saying that if I am having n features obviously in our example it is four. So, we are having x one two x n features. Then, which is the best feature to make the split on, so this is the first question. Okay. Now, second question is only important. So first question is Is it easy to understand second question is only important when you are having a continuous feature. Now, once we have decided say, I want to make a split on x one, so I know I want to make a split on x one, x i sorry.

Now, the next question arises, what is the value that I should split this particular feature on. Suppose a situation where xi is representing height of individuals. Now, in that case, if it is representing height of individuals. Then in that case. Well, should I need that split on for a categorical data, let's say, I am splitting on windy or humidity, it is easy to split on these two values, or if I had chosen this outlook, then I would have splitted this data into three parts a rainy overcast and sunny. It is quite easy because the number of unique values are distinct, whereas for the case of height, it would not be unique, and that is why the problem arises for a continuous data, then how do we solve this particular issue, we solve this issue by discretizing the continuous variables and what do we mean by discretization of continuous variables. What we do is we put all those continuous video, all those continuous values in form of bins, we define an interval and we put them into bins and and uh once we put them put continuous variable into bins, then the continuous variable turns into a categorical variable, where each bin represent one category. So I hope you are able to understand what is the issue with a continuous variable and how we might be able to solve that particular issue. Now let us move on and discuss about information gain.

Okay, so the first question was, that if I am having n features, then how am I going to make sure that which feature, should I start from. Okay, so that cushion can be solved using this information gain concept. So for every node or for every split, when when we are starting split we are having a data set. Okay, so every data set will have something known as impurity. Okay, let us first not talk about this impurity first of all let let us talk about what do we mean by information gain so information gain is just a metric. So just like in logistic regression we have cost functions in linear regression we have cost functions, which we optimize to trying to reduce that that cost function to kind of come up with the best values of our weights in the similar manner in decision tree we have this information gain and using this information gain, we come up with the best feature to start our splitting process on. Okay. So obviously, we want our information gain to be as high as possible. Now how does it work.

The way it works is we want to choose which feature to choose. So I'm having a let's say I'm having this full feature. So I will compute information gain for Outlook, I will compute information gain for temperature, I will compute information gain for humidity and windy. I will choose that particular feature for which information gain will be maximum. Okay, so I hope you get the idea of how does it work. Now let us talk about how does information gain actually work, so to talk about that, I would like to discuss the concept of impurity. Now what do we mean by impurity, I would like to take up an example. So you will be able to understand it very clearly and be able to remember this very vividly. Okay, so let us start talking about that. Now, what I have done is I have taken an example where I have taken two sets of data set one and set two. And in each set. I have taken, I have assumed that key in the second one, we will be having number of rows belonging to class A will be hundred and number of rows belonging to class B, sorry it should have been B.

Okay. So number of rows, belonging to class A B will be okay. And in the set to set to is another data set. And in that set, we are having a number of number of rows belonging to class A is hundred and number of row belonging to class B is 10. Now my question to you is that, in which set out of these two sets in which set, you are more comfortable in predicting a random data point say for example someone comes to comes to you and ask you that this is a random data. Now, in which set, you will be more likely to predict it correctly. Obviously, you will be able to predict it more correctly in set two, and the reason is that in the set two. There are there are more chances of you being correct because most of the examples are already belonging in one class, and a very few examples are belonging in the second class. So that is one, if you just randomly say that this particular class belongs to the belongs to class A this particular example, then you will be more, most likely, correct. Okay, so I hope you get the point. But whereas in the set one, you are ready 50 50% sure as to you are unsure, very uncertain as to where this particular new example will belong. Okay, so this is what I have asked here. What do you think about each of these data sets. So, so let us now move on.

Okay, so this is a impurity concept continued. Now, now I have just answered that question key. It will be second set, because in this set, most of the examples. Most of the examples are already belonging to the class A, and hence this data set will be more pure than this data set to one. So this is what we mean by purity. In this particular example, first set to this particular set will be considered more pure because we are certain as to where most of the distribution of class lies. Okay, but for set one we are not that much sure of the distribution, right, and that distribution is uniform. Okay.

Here, distribution is one sided it favors the class. A. Okay, so that is why we will say that set two is more pure than set one. Okay. But we need a mathematical way or a mathematical metric, by which we can compute purity. And that is why, different type of purity measures have been discovered, or and have been are being used. And in this particular class, I want to show you a one such measure which is known as guinea impurity measure, and the formula of Guinea impurity measure is this. Now he someone. If you are if you are getting intimidated by this formula. Then, do not get it is quite easy. Now, let me break this particular formula down first of all, do not pay attention to this. This particular part only pay attention to this particular formula and understand that this C is the number of class that we are heavy in our earlier example, the data set that we do this CS two, so we are having two classes. And this P is the proportion of examples belonging to each class. Now what we have just done is we have computed the proposed for a particular data set we have computed the proportion belonging to the class of. Four in favor in favor of a particular class and against a particular class so for example I am having two classes as a and b. So, what I will do is I will split the data set into these two classes, so I will, I will subset a data, where all the example belongs to A, and I will subset the data, where all the examples belong to B, and I will compute the proportion of that. A and B. And what I will do is I will take that proportions, square. Notice here, I am taking this square and I will sum that. Okay, those two values, and I will subtract that from one. So this is the whole idea of impurity. Now what do you think impurity larger the impurity, the better it is. or. It is worse. Think about it, impurity is a measure, which we want to decrease. So we want to have less security. Now how does all these fit in with information gain we are going to be discussing in later slides. So first of all make sure that you understand this particular slide as clearly as possible. Now let us move on. Now, the next thing is concept of impurity continued. Now what I've done is I have taken this these set one and set two, and I've computed this Gini impurity for them. So for set one impurity will be one minus no notice here.

I have subtracted two terms. The reason is that there were only two classes, and for each class I have taken the proportion, so there were 100 examples belonging to first first class and hundred examples belonging to second class. So, we have just taken the proportion and here we squared it, and the answer will be 0.50. Okay. And for set two. There were hundred observations belonging to class one and 10 observation belonging to class two. And here are our answer is 0.165. And notice here, the impurity is less for set, which clearly demonstrates and shows that a set two is less impure than set one. Okay, now let us talk about how does impurity concept fit in with information gain.

So, what do we do is, we, we first of all, assume a data set. So we start with a data set, and that data set, you can think of a data set in the root node, and we want to split this data set on some feature say excite. Okay, now what we do is, we assume that, let's say, Xi has four unique values. So we split that data set on those four unique values. So resulting data set will be for a subset of data D one D two D three, and D four. Now we have a D data set. And we have a D sub data set, if I split this particular data on exci feature. Okay, now what I can do is I can compute impurity for D, and at the same time I can compute the impurities of these a bunch of data set. Now what I can do is I can define information gain like this. Read this particular line.

So what I'm doing is I'm computing the information gain for XIV.

And I'm taking the impurity of D. So, the data set that I started with, and I took the impurities of some of impurities of these sub datasets. If I split this particular feature on x i and this difference is what I would like to maximize. Now, you need to understand why does this particular formula makes sense. The reason it makes sense is, If I take impurity of data D. And if I'm taking the impurities of data. Now then, we'll, this will be a maximum. This will be maximum when this sum of impurities will be minimum, because I'm subtracting this, right. So, if this sum of impurities will be minimum, that means, this particular excite feature is the best feature that I have to split on. Now, I will do this information in step four all the features. So for our example we are having four features. So I'm going to compute this information gain four x one x two x three and x four, and I will choose that feature for rich this information genius maximum. So I hope you got this particular discussion on information gain in terms of impurity. Now let us move on. Now, we are going to be briefly, talking about how does exactly decision tree algorithm work, it is quite easy to understand and in fact it is a one of the most easiest easiest algorithms to, to understand intuitively. So, we have got n features, and we have a variable to produce a y. So what we do is we first of all look through all the features. Okay. And for each feature, what we do is we first of all compute its information here, and they store this information gain in a list. So in the first system we what we are doing is we are essentially trying to come up with the best feature to split on. Okay. That is what is happening in the first step, then what we do is we choose that feature for splitting for which the information gain is maximum. Okay, so we understand this step also. So in the second step we are actually taking that information gain for which it is maximum, for which information in gaining gains maximum. And once we decide which feature to split our data set on then other thing is easy because we have to then recursively repeat these two steps first two steps. Okay, so then what we do is let us let the above features split the data into case of nodes. So, it to actually make sure you understand the third point let me go back to this data. And let us say we decided that we are splitting our data on Outlook. Now, if I'm deciding that I'm splitting my data set on Outlook, I want you to answer a question that how many sub parts of data, we will be getting, we will be getting three sub parts and the reason is that because outlook is having three unique values. Okay. So, we will be getting that three sub data. So what we are essentially trying to do in the third step is. So, we have assumed that you assume that the above feature that we choose from the second step, which which is Maxim which is having maximum information here. We are choose that particular feature, and we assume that that feature splits the data into K sub nodes. So essentially what this step is doing is, it is splitting this data into K sub data's. Okay, now what we do is for each sub data, we are repeating these two sub lists, these two steps one and two recursively. Okay, so we, again, split this first sub data into two parts, second sub data into two parts, and third sub data into two parts sewn up to key sub data. Right. I hope you're getting the point until I until we get a perfect split. So until a sufficient criteria has been satisfied or not. Okay, so now let us understand this decision tree algorithm in the form of pseudocode so that you will be able to understand what exactly we are able to come up with. Okay, so the first thing that we are going to be doing is we are going to be defining a way of computing this Gini impurity. Okay. And the way we are going to do that is we are going to be taking up an input called data set, and we are going to be computing a first class proportion, and first second class proportion. And the reason these things are important because earlier we discussed that in the Gini impurity. We need to come up with these proportions. Okay. And that is why we are referring to these proportions in the form of P one and P two. And why are we taking only two proportions because. In our example, cb two. Okay, so.

Okay, so first we define Guinea impurity for a particular data set. And once we have defined the Gini impurity, we are going to make a function, which is going to decide which feature, we need to split our data set on. So, given a data. We need to decide which is the feature, and essentially what this function will we are tackling is this function is we'll be tackling these two steps.

First two steps, it is going to decide which features Should I split on, obviously, that will be decided by a maximization of information gain. Okay, so this whole pseudocode should make sense why because first of all, we are deciding, we are making an empty list for storing information again. We are going to obtain the impurity of data without a split. And why does this particular step make sense. The reason we are obtaining this. This impurity without a split is because information is in gain is what information gain is impurity of D without a split, so we are having this data D, and we are having further details. D one D two D three d four these four sub data's, we are essentially subtracting this sum of weighted impurities of D one D two D three now for the one, one more thing that I need to that I need you to notice this weighted impurities. I would like to show you this thing, or once I started coding. So right now you don't need to worry about these fated. Okay, so we have impurity of D and we have some of impurities of D one D two D three. So, that is why we are first of all, making a making a variable for the story impurity of data without a split, and we loop through the columns we loop through all the columns, essentially we are looping through all the features and for each unique value of column. So, for a single column, I will loop through all the unique values, I will compute its impurity, and I will compute the information gain and append it to information gain. Okay, this list, I'm going to open, and I'm going to return that feature for which information gain was maximum. So this is the whole idea of this particular example, and finally, in Step three, I'm going to call this feature to split on function so this function I'm going to call, and I'm going to call it recursively because the third step in our earlier pseudocode was to call these two steps. First and do recursively. So that is what I am doing in the third step here. Okay, so I hope you get god this particular point of pseudocode because once you got this particular point of pseudocode, now what we are doing is we are essentially trying to come up with that, that we are going to actually start coding the ultimate, so.

Okay, so let us now move on.
Alright so the second, the next step is coding the decision tree algorithm in Python. So to do that, the first thing that I have to do is, I have to load the data set that I showed you in the first slide in my Jupiter lab environment. So, I have already done that for you. So I have got this data set. Go out or not.

data set. Now this data set, I want to use. So that is why we are going to be using this data set because we discussed it in the first slide only, and it is easy to data, it is easy to use data set. The reason is that all the features are categorical in nature. So it will be easier for us to understand actually what is going on inside the algorithm. Okay, so let us now create an empty Python three notebook. And let me rename this Python three notebook. As

decision tree. Okay. So, once we have renamed this. This particular notebook. Let me import necessary libraries.

Okay import pandas as PD import NumPy as NP.

And let me quickly load the data, dt dot, read CSV.

I'm going to import. Go out or not.

And let me look at this particular data set really quick. All right, so we are having this particular data. So we are going to be following these steps that we discussed in the pseudocode to actually code this whole algorithm. So, the first step is to code the Ginni impurity function. Now what this Ginni impurity function is going to take is, it is going to take a random data set, and it is going to return this formula representation and why are we using this formula because earlier we discussed this ginni, dirty formula. Okay, so let's do that really quick. I'm going to say, define Gini impurity, and it is going to take data set. And once this data set has been passed. I'm going to say first is equal to. And I'm going to say data set.

And I'm going to

extract this go to manual, and I'm going to say how many variables are having yes to call us corresponding to this quote variable. So whatever number of two we have, we are going to sum those number of variables. And finally, we are going to be dividing it by data set, dot shape and first argument. So if any of you is not familiar with this particular data set, dot shape. Let me show you on this data. Okay so data dot shape is 14 comma five, and if I extract the first first real value, then it is 14 so I'm essentially dividing it by 14 to make sure that I am correctly getting the correct proportion, and I'm going to do that again for second proportion.

Okay, but this time I'm asking for No. Okay. And finally, I'm going to return one expression, which is power of first to

second to. And one more thing I just remember that earlier in my pseudocode I have, I have called these variables as p one and P two so to be consistent with my pseudocode let me, rename this SP one, and P two

p one, and P two. Okay, so we have got this, gimme impurity function. And let me quickly call this Guinea impurity function on my old data set.

All right, so the Gini impurity that I'm getting is 0.459. Now, if you just look at this particular number and want to interpret something then it is not going to work because Gini impurity works only by comparison, so you have to have some other data set, which kind of compare how pure, or how impure these two data sets are in comparison to each other. So earlier also when we took this example, we took two data sets, we just did not computed the impurity for a one data and we just said, this is the impurity No, we computed impurity for two data, two different data sets, and we, we kind of compared, how does how does these two impurities, compared with each other and, oh, what, which data set is more pure, pure then the other data set. Okay, so that is what, what is the idea so we are we are done with the first step, we have successfully coded this particular impurity let me do one more thing. Let me make a sub data set so sub data set, which will be data. Let me do one thing. Let me extract the data, where temperature is hot. Okay, so I'm going to say data dot temperature.

I'm going to say where it is hot. And let me print this data. So, I have extracted this data where are temperature is hot. Now let me do one thing. Let me call the name dirty on this sub data. Now notice this sub data is having higher impurity than the whole data. So obviously, this split so if I split this particular data and temperature then it is not going to give me the better results. So this is the whole idea of impurity. Okay, I hope you get this particular point. Now we are going to be executing the second function, and that second function is what it is, essentially trying to accomplish is trying to accomplish and come up with a best feature to split on, so it is obviously going to use this new impurity function. So, let us now make this particular function. So I'm going to say, define.

And I'm going to call it, which

which feature to split on.

Okay. So, it is also going to take up a data set. And first thing that I want to do is I want to make a list as information gain. So this list is going to store all the weighted values of impurities, or other reduction in impurities because information gain is defined as the reduction in impurity. Okay, so I'm going to say impurity.

Without split.

And I'm going to call this Guinea impurity function on this whole data set that I paused in here. Okay. and next thing is I want to loop through all the columns, so four columns in

data set.

dot.

And we will use columns. But remember, this data set the dot columns is going to return all the columns, but we don't want that last one. Okay, so what I'm going to do is I'm going to extract it all the way till last one. And now I'm inside in each column. Now I want to get the unique values of each column. So I'm going to say, unique values of current column, and to get that what I am going to do is I'm going to do, data set

data set. And I'm going to extract the current column, which I have which I have access to using this, and I'm going to get the unique of it like this. Okay, once I get the unique, I want to initialize impurity, which is going to zero, and I'm going to start looping through these unique values for you so I'm going to do four unique in unique values.

Okay, and now I'm going to extract the current detector, on which I'm going to I'm going to call this new impurity again. Okay, I'm going to do.

So how are we going to make this current data. So we are having the access of column, and we are having the access of unique, so the way I'm going to do that is, I'm going to say data set data set. Column current column, and I'm going to say, which is equal equal, unique, so I want to get this current data like this. And once we get this current data. Now I want to compute its impurity. So how are we going to compute its impurity. We are going to say, the current impurity.

And for that, I'm going to call the name purity on this new dataset corrupted data.

Okay, so uh once we have this, we get. But first of all let me make this impurity.

Okay, so once we have this. Now what we want to do is we want to add the impurity in this current impurity but we don't want to add the impurity in the similar manner. Like, vv complete computed this current impurity, but we would like to append the impurity, as a weighted impurity. So the way we are doing going to do this by is. We are going to take the shape, take the number of values in the current data so number of rows in the current data I'm going to say current data dot shape, and I'm going to extract the second column. First, and I'm going to divide it by a whole data sets value.

And finally, I'm going to multiply this current impurity. Okay, so this way, we are going to have weighted impurities. And finally, once I get out of this loop I want to append this to information in dot append this impurity value.

Alright, so once we append this information gain.

But notice here I have just done a mistake because I'm just appending this information value, but rather what I should do is I should subtract this impurity without splits subtract this impurity value from this impurity without split, so let me do that really quick impurity without split subtracted by impurity value. Okay. And finally, I'm going to return what I'm going to return our data set.

dot columns. And let me get all the four columns, and I'm going to get what I'm going to get there. The information gain is maximum.

Okay, so I have done a np.org next. Now this is going to return me the index for which this, and information gain is maximum. And I want to get that particular index Okay, so this is going to work properly.

Let's run this. Now let me call this rich feature to split on function on this whole data.

Alright so as you can see, we are getting outlook. And the reason we are getting Outlook. Outlook is the first feature. So, it must be having the largest information in this particular feature. Okay. So, how are we going to verify this, so let me have one. Do one more thing. Let me return one more variable with it. Let me return information gain also.

And let me run this thing again. Okay so this time, you can clearly see, we are having this information again and we are having maximum information gain from this outlook variable.

Okay. So, let me get rid of this now.

Okay, so the next step is to.

Next step is to actually. So let's look at our pseudocode.

Okay, so the next step is to finally call this, which feature to split on recursively. So how are we going to do that. So we are going to make a function called run decision tree for that. Or rather, we could just run this particular function without creating any function, right. So, what we can do is we can, we can begin define a function called run decision tree.

And it is going to take mean data as one input.

Okay, and. First thing, and let me now, get rid of this information again because we don't need this anymore.

Okay, so now what I'm going to do is I'm going to first of all, call this which feature to split on on this main data.

Okay, so this way, we get the feature. And next we are going to print a statement, and that is paid to a statement will be split the data on what I'm going to split this data on this feature. Okay, so this will be the first statement, and the next statement will be what the next thing that we need to do is we need to recursively call this which feature to split on on the sub data sets. So first of all we need to get all the unique values of this current feature so the current feature, as we earlier saw it was outlook, so I need to get the get get all the unique values for this outlook valuable so I will do for unique in main data.

And I'm going to extract the, the feature that we received from this function. And I'm going to call this unique on it. Okay.

All right. So we have got this particular.

We have got this particular loop and in each loop. Now I want to do.

Current feature.

And in this current feature I'm going to call which feature to split on on what I'm going to call it on this sub data set that we got. And which one is the sub data set for these unique values. So I'm going to do main data.

And I'm going to do feature.

Let's see. Main data.

And I am going to do feature so current split is on this feature, and I'm going to say if this feature is equal to this unique. Okay, so we have got this sub data in this form. And once we get the current feature. Now we want to compare this current feature. So if this current feature is equal to. This feature. Then we have we want to make the decision. Notice this thing try to understand it very carefully. If this current feature is not equal to this feature, then we want to continue splitting because let's say I have got outlook the outlook variable to split on, and here also I get the outlook. That means this outlook is probably the best, and we have to probably make the decision. Okay, so we want to do something like if current feature is not equal to feature. So if it is not equal to feature. Then I'm going to run decision tree so I'm going to call this particular function again.

And I'm going to call it on this sub data.

Okay. And further, but before doing that. I have to make another print, print statement, and here I'm going to do something like for.

I'm going to say for feature.

And what why am I am doing this because I'm inside this feature right now for feature.

I'm going to say equal.

And

let's see, for feature equal to what for feature equal to this unique value. And I'm going to do something like split on split on what whatever we have got as a current feature. So for feature is equal to, unique value I'm going to split on current feature.

Okay, I'm going to close this print function. And finally, If it is not, if it is indeed equal, then I'm going to make the decision. So I'm going to say print.

I'm going to say for.

I'm going to save feature for feature,

unique, so again I'm making the same statement but this time I'm going to I'm going to stop splitting and I'm going to predict. So predict what I'm going to extract.

So I'm going to save main data.

Main data and feature.

It will call unique.

Okay, so we have got this. And finally, I'm going to extract the go out variable.

And I'm going to get the most out of it.

Okay, so more. And after taking more, I want to get the bad news. Because, once I call this mod function. It is going to return me a NumPy array, which is, which can be from which I can get the first value of NumPy array. Okay. So I hope this function is going to run successfully. So let's run this particular function and let's run this decision tree on our main data. Main data was what main details data. Alright, so we are getting this decision tree expression. Now, let us try to infer the way it is going to work. So, it is, first of all splitting the data on Outlook. Okay, now outlook is having. So let's look at the outlook.

Now, outlook is having these three variables, rainy overcast and sunny. So for each of them, it is going to make further decisions, so far for outlook is equal to rainy, split on humidity. So further it is going to say, Well, if outlook is rainy, then split on humidity. Okay. And then it is saying split the data on humidity so it is making the same statement again. and it is saying if humidity is high, then it split on Outlook. Alright, and if that's fear, then again I'm going to split on Outlook, and if outlook is rainy, then predict No. So the way we are going to make a prediction from this decision tree. Now, obviously this decision tree is not a nice representation, because we are not able to read. Clearly as to the way it is making predictions, but you need to understand the way it is going to work. Okay. For any new example for any new example it is going to ask these bunch of questions and it is going to successfully predict which one. Our particular example belongs to. So I hope you got this particular point. Now, hopefully, we need, we don't need to do all these things because we are having libraries like a scalar, which makes us makes our life really easy by a coding all these things up for us. We don't need to do all these things but we need to understand the way this decision tree is going to work. Okay, so, to kind of demonstrate you. So what I'm going to do is I'm going to run this decision tree from a scalers decision tree classifier. So I'm going to say from SK learn dot tree. Import decision tree classifier. Okay. But to call this decision tree classifier, I need to have the data set in the proper format. Okay. The reason that I need to have the data set in a proper format is because if you look at it. If you look this particular data set. Currently we are having text values, although this is categorical but SK learn by default does not understand this particular text values. So, we have to map these values, how are we going to map. So I, I need to map, all of these values because there every column in this particular data set is a categorical one, so I can probably do that by, by running a loop. So I can say full column in data dot columns. And I'm going to say data.

So, before that let me.

Print the data and kind of see it looks like this and after this loop it is going to look something different. So data.

And I'm going to call map on it, and to call the map on it, I need to pass a dictionary, which is going to be a dictionary of value key pair for key value in enumerate

data and columns. And I want to get the unique of it. Okay. So, let me run this particular. Okay. It is in columns not defined. When I say columns.

It is a key.

It is defined why the saying, or columns not defined. So where Oh, yeah, so I got it, so it is where the error is occurring. So let me run this again. Yeah. So now if I print the data, you can clearly see, it has successfully coded itself. Now I need to define x&y, so I will say data.

Data dot columns so I want all the columns in it, except the last one. And I'm going to define data. Data columns and for y i just need the last one. Okay. So we have these values. So let's print these values so we are having these four features, and we are having one variable that we will want to predict. And finally, I want to make a classifier object so CLF, and I will call it decision tree classifier. And finally, I'm going to fit this classifier on x and y. So yeah, so it is going to work properly, and I can now use this classifier to predict our x. So we tell it is predicting this and if I compare this prediction using. So let's subtract away from it. So we are getting 000, which means we are getting 100% accuracy for this particular model. Okay. So I hope you this, you got this particular point and you can clearly see from here also, that how easy decision tree classifier makes our life, because we don't have to code all those things that we have coded. Just now, and we we obviously we can make this a little bit more readable, because right now. The main problem is, it is not readable, but obviously we can do that but still it is looking quite nice.

But most of the things we just need to understand how does decision tree algorithm most of the things are already achieved using this decision tree classifier from inbuilt SK learn library. Okay, so I hope you got this particular point.

So now let us move on to the next slides, which, which is having a task for you. So decision tree classifying as killer. Now that task is that you have to get access to this particular data set and this data set is a classification problem of job postings. So, this particular data set lists all the job postings on a particular website and your task is to predict whether that particular job posting is real or fake. So this is a classification problem. And what I want you to do is I want you to run a decision tree classifier. On this particular data set using SK learn library. Okay. So I hope you will be excited to do do this particular task. And finally, we are done with this particular video. Thank you so much. have a nice day.
