{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TutorialFile.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "BWxV5S7p6RAj"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "# How to Create Wikipedia Content Classification web application in Python?\n",
        "\n",
        "Hello Eveyone,\n",
        "\n",
        "In this article, we are going to learn to create a Wikipedia Content Classification Application. Before we actually go into the theory and learn to create this application, let us quickly see how this application looks like and how it works. \n",
        "\n",
        "After, running the Final application, you will be greeted with the following web interface. \n",
        "\n",
        "![Wikipedia Content Classification Web Applicatioin](https://raw.githubusercontent.com/datageekrj/FiverrContentRepo/master/wikipedia%20Content%20Classification/intro.PNG?token=ANKECZZ4PT2CGSCGA3IKQUK6WUUN4)\n",
        "\n",
        "Read each and everything that is getting displayed on the image above. \n",
        "\n",
        "You should notice that the `Enter URL` field in the image. This is a place where the actual URL of the wikipedia page needs to entered by the `USER` of the application (whoever is using the application). \n",
        "\n",
        "When, we submit some URL in the that field, the following process will happen in the backend of the script:\n",
        "  * The `request` module will get the content of the page corresponding to the URL that the user has entered\n",
        "  * The `BeautifulSoup Module` will make the soup object and will get all the text of that wikipedia page for that `URL`\n",
        "  * The model that we have trained using `sklean` (we will learn how to train model in this tutorial) in the backend will be called on the scraped text and we will get the prediction.\n",
        "  * Them, that prediction will get displayed in the application. \n",
        "\n",
        "Let us assume that we enter a URL of the Tom Cruise Wikipedia Page and Hit Submit, then the display will look something like:\n",
        "\n",
        "![Wikipedia Content Classification Web Applicatioin - Prediction on Top Cruise Wikipedia Page](https://raw.githubusercontent.com/datageekrj/FiverrContentRepo/master/wikipedia%20Content%20Classification/tom.PNG?token=ANKECZ7XUV2BOVNE5R4LTQS6WUUQM)\n",
        "\n",
        "Then, we can go back and check the prediction of another web pages. It is a fun application. Isn't it?\n",
        "\n",
        "So, this is what we will be building in this tutorial. So, buckle up..\n",
        "\n",
        "I hope this small discussion on how the wikipedia classification application works makes sense to you. Now, that we understand what this application is and what are we building, we can actually get down to the steps of building it actually. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWxV5S7p6RAj",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU: \n",
        "## TASK NUMBER. 1 What do you think of a web application? Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD325HL96fhm",
        "colab_type": "text"
      },
      "source": [
        "### Answer\n",
        "\n",
        "A web application is just a normal website with some special purpose. We can have a web application for booking flights, or to watch live Tv shows etcetra. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QvJBqX8_Bctk"
      },
      "source": [
        "# Steps to create this Application...\n",
        "\n",
        "**Learning Objectives:**\n",
        "  * Creating our own `data` using web scraping with *Beautiful Soup* \n",
        "  * Once data has been scraped and created, cleaning and labelling the data\n",
        "  * Creating a Pipeline for the Model using Multinomial Naive Baye's Algorithm\n",
        "  * Finally, using the pipeline model, create a flask application and predict the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TIFJ83ZTBctl"
      },
      "source": [
        "# STEP.1 GETTING THE DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s_JOISVgmn9v"
      },
      "source": [
        "## Importing the necessary Modules\n",
        "\n",
        "Before we actually start the actual coding process, we need to first import the nessary modules that we will require in the building of the application.\n",
        "\n",
        "**Before we import any modules, I have included one code that installs all the necessary modules for this tutorial.** \n",
        "\n",
        "If you are sure, you can skip running this line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R7yotKI4zxW",
        "colab_type": "code",
        "outputId": "e7458a58-d41b-41e7-875e-79e0c188eb0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "!pip install requests\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install bs4\n",
        "!pip install sklearn\n",
        "!pip install flask==0.12.2 \n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests) (2020.4.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.3)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4) (4.6.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.18.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Collecting flask==0.12.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/77/32/e3597cb19ffffe724ad4bf0beca4153419918e7fa4ba6a34b04ee4da3371/Flask-0.12.2-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2) (1.0.1)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2) (7.1.2)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.2) (2.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.4->flask==0.12.2) (1.1.1)\n",
            "Installing collected packages: flask\n",
            "  Found existing installation: Flask 1.1.2\n",
            "    Uninstalling Flask-1.1.2:\n",
            "      Successfully uninstalled Flask-1.1.2\n",
            "Successfully installed flask-0.12.2\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.23.0)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (0.12.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: click>=2.0 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.21 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.4->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue3nBbvn8mnk",
        "colab_type": "text"
      },
      "source": [
        "Once, we have installed all the necessary modules, we can easily import them. \n",
        "\n",
        "Note, here that some modules that we have imported below are not installed above. Why is that?\n",
        "\n",
        "# TASK FOR YOU:\n",
        "\n",
        "## TASK NUMBER.2 Why do you think that some modules were not installed earlier but they have been imported below? Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wabIAojPl3L8",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czpDym7e9Coh",
        "colab_type": "text"
      },
      "source": [
        "The answer to this question is that some modules comes pre built with python. Hence, they are not required to be installed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msqsTnyT9uCi",
        "colab_type": "text"
      },
      "source": [
        "# Importing the modules\n",
        "\n",
        "Now, we can import the necessary modules which we will be using to create the web application"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aSRYu62xUi3g",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import re\n",
        "from flask import Flask,request\n",
        "from flask_ngrok import run_with_ngrok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "daQreKXIUslr"
      },
      "source": [
        "# Let us briefly explain the objective of each of the above module\n",
        "\n",
        "**Module Explanation:**\n",
        "  * `requests` is for getting the content of the *URL* \n",
        "  * `pandas` is for making a data frame of the text\n",
        "  * `numpy` is just for doing some basic computation on the datasets\n",
        "  * `bs4` for converting the content that we get from requests.get to soup object which makes it easy for querying the `HTML`\n",
        "  * `pipeline` from `sklearn` is for building a pipeline of the model. \n",
        "  * `sklearn` for training the `MultinomialNB` and preprocessing of the data\n",
        "  * `re` for some text parsing and regular expression function for data cleaning.\n",
        "  * `flask` for creating the actual web application\n",
        "  * `flast_ngrok` for running the web server using internet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-94KEbgOM5rC"
      },
      "source": [
        "Now, that we understand what each of the modules do that we just imported, we can start coding this application. \n",
        "\n",
        "First, let us create a genral function which will accept the url of any wikipedia page and is going to return the text content of that page. \n",
        "\n",
        "## Given any URL, the following function will return the text content of that wikipedia page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KLS42pRgN4W4",
        "colab": {}
      },
      "source": [
        "def get_wiki_data(url):\n",
        "  page = requests.get(url)\n",
        "  html = page.text\n",
        "  soup = BeautifulSoup(html)\n",
        "  content = soup.find_all(\"p\")\n",
        "  data = []\n",
        "  for paragraph in content:\n",
        "    data.append(paragraph.text)\n",
        "  data = \" \".join(data)\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ju6yF3xaN_GJ"
      },
      "source": [
        "Before understanding what is actually going on here, let us first run this function to see if it works as expected. We can pass the URL of any wikipedia page and get the content (content means all of the text content) of that wikipedia page. Let us check this function for the Machine Learning Web Page URL [Machine Learning](https://en.wikipedia.org/wiki/Machine_learning)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rjqWhAN4N7Ah",
        "outputId": "7dcce90d-d3c7-440e-a587-2be3a97db6a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "get_wiki_data(r\"https://en.wikipedia.org/wiki/Machine_learning\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.[2][3]:2 Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop  conventional algorithms to perform the needed tasks.\\n Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5] In its application across business problems, machine learning is also referred to as predictive analytics.\\n Machine learning involves computers discovering how they can perform tasks without being explicitly programmed to do so. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required  to solve the problem at hand; on the computer\\'s part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than have human programmers specify every needed step.[6][7]\\n The discipline of machine learning employs various approaches to help computers learn to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset has often been used. [6][7]\\n \\nEarly classifications for machine learning approaches sometimes divided them into three broad categories, depending on the nature of the \"signal\" or \"feedback\" available to the learning system. These were:\\nSupervised learning:  The computer is presented with example inputs and their desired outputs, given by a \"teacher\", and the goal is to learn a general rule that maps inputs to outputs.\\nUnsupervised learning:  No labels are given to the learning algorithm, leaving it on its own to find structure in its input. Unsupervised learning can be a goal in itself (discovering hidden patterns in data) or a means towards an end (feature learning).\\nReinforcement learning:  A computer program interacts with a dynamic environment in which it must perform a certain goal (such as driving a vehicle or playing a game against an opponent) As it navigates its problem space, the program is provided feedback that\\'s analogous to rewards, which it tries to maximise. [3]\\n Other approaches or processes have since developed that don\\'t fit neatly into this three-fold categorisation, and sometimes more than one is used by the same machine learning system.  For example topic modeling, dimensionality reduction or meta learning. [8] As of 2020, deep learning had become the dominant approach for much ongoing work in the field of machine learning . [6]\\n The term machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence. [9][10] A representative book of the machine learning research during the 1960s was the Nilsson\\'s book on Learning Machines, dealing mostly with machine learning for pattern classification.[11] Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. [12] In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal. [13]\\n Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E.\"[14] This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing\\'s proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\".[15]\\n As a scientific endeavor, machine learning grew out of the quest for artificial intelligence. In the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics.[16] Probabilistic reasoning was also employed, especially in automated medical diagnosis.[17]:488\\n However, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.[17]:488 By 1980, expert systems had come to dominate AI, and statistics was out of favor.[18] Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval.[17]:708–710; 755 Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.[17]:25\\n Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory.[18] As of 2019, many sources continue to assert that machine learning remains a sub field of AI. Yet some practitioners, for example Dr Daniel Hulme, who both teaches AI and  runs a company operating in the field, argues that machine learning and AI are separate. [7][19][6]\\n Machine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on known properties learned from the training data, data mining focuses on the discovery of (previously) unknown properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to reproduce known knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously unknown knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\\n Machine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.[20]\\n Machine learning and statistics are closely related fields in terms of methods, but distinct in their principal goal: statistics draws population inferences from a sample, while machine learning finds generalizable predictive patterns.[21] According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.[22] He also suggested the term data science as a placeholder to call the overall field.[22]\\n Leo Breiman distinguished two statistical modeling paradigms: data model and algorithmic model,[23] wherein \"algorithmic model\" means more or less the machine learning algorithms like Random forest.\\n Some statisticians have adopted methods from machine learning, leading to a combined field that they call statistical learning.[24]\\n A core objective of a learner is to generalize from its experience.[3][25] Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\\n The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\\n For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.[26]\\n In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\\n The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.\\n Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.[27] The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and the desired output, also known as a supervisory signal.  In the mathematical model, each training example is represented by an array or vector, sometimes called a feature vector, and the training data is represented by a matrix. Through iterative optimization of an objective function, supervised learning algorithms learn a function that can be used to predict the output associated with new inputs.[28] An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.[14]\\n Types of supervised learning algorithms include Active learning , classification and regression.[29] Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. As an example, for a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. \\n Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\\n Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms, therefore, learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, such as finding the probability density function.[30] Though unsupervised learning encompasses other domains involving summarizing and explaining data features.\\n Cluster analysis is the assignment of a set of observations into subsets (called clusters) so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some similarity metric and evaluated, for example, by internal compactness, or the similarity between members of the same cluster, and separation, the difference between clusters. Other methods are based on estimated density and graph connectivity.\\n Semi-supervised learning falls between unsupervised learning (without any labeled training data) and supervised learning (with completely labeled training data). Some of the training examples are missing training labels,  yet many machine-learning researchers have found that unlabeled data, when used in conjunction with a small amount of labeled data, can produce a considerable improvement in learning accuracy.  \\n In weakly supervised learning, the training labels are noisy, limited, or imprecise; however, these labels are often cheaper to obtain, resulting in larger effective training sets.[31]\\n Reinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov Decision Process (MDP). Many reinforcement learning algorithms use dynamic programming techniques.[32] Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\\n Self-learning as machine learning paradigm was introduced in 1982 along with a neural network capable of self-learning  named Crossbar Adaptive Array (CAA). [33] It is a learning with no external rewards and no external teacher advices. The CAA self-learning algorithm computes, in a crossbar fashion, both decisions about actions and emotions (feelings) about consequence situations. The system is driven by the interaction between cognition and emotion. [34]\\nThe self-learning algorithm updates a memory matrix W =||w(a,s)|| such that in each iteration executes the following machine learning  routine: \\n It is a system with only one input, situation s, and only one output, action (or behavior) a. There is neither a separate reinforcement input nor an advice input from the environment. The backpropagated value (secondary reinforcement) is the emotion toward the consequence situation. The CAA exists in two environments, one is behavioral environment where it behaves, and the other is genetic environment, wherefrom it initially and only once receives initial emotions about situations to be encountered in the  behavioral environment. After receiving the genome (species) vector from the genetic environment, the CAA learns a goal seeking behavior, in an environment that contains both desirable and undesirable situations. [35]\\n Several learning algorithms aim at discovering better representations of the inputs provided during training.[36] Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\\n Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization[37] and various forms of clustering.[38][39][40]\\n Manifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors.[41] Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.[42]\\n Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\\n Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately.[43] A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine the class to which a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.[44]\\n In data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.[45] Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.[46]\\n In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.[47]\\n Three broad categories of anomaly detection techniques exist.[48] Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherently unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set and then test the likelihood of a test instance to be generated by the model.\\n In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies and imitation.\\n Association rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\".[49]\\n Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.[50] Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\\n Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets.[51] For example, the rule \\n\\n\\n\\n{\\n\\no\\nn\\ni\\no\\nn\\ns\\n,\\np\\no\\nt\\na\\nt\\no\\ne\\ns\\n\\n}\\n⇒\\n{\\n\\nb\\nu\\nr\\ng\\ne\\nr\\n\\n}\\n\\n\\n{\\\\displaystyle \\\\{\\\\mathrm {onions,potatoes} \\\\}\\\\Rightarrow \\\\{\\\\mathrm {burger} \\\\}}\\n\\n found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\\n Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.[52]\\n Inductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.\\n Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.[53][54][55] Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.[56] The term inductive here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\\n Performing machine learning involves creating a model, which is trained on some training data and then can process additional data to make predictions. Various types of models have been used and researched for machine learning systems.\\n Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\\n An ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer) to the last layer (the output layer), possibly after traversing the layers multiple times.\\n The original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\\n Deep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.[57]\\n Decision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item\\'s target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.\\n Support vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.[58]  An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\\n Regression analysis encompasses a large variety of statistical methods to estimate the relationship between input variables and their associated features. Its most common form is linear regression, where a single line is drawn to best fit the given data according to a mathematical criterion such as ordinary least squares. The latter is often extended by regularization (mathematics) methods to mitigate overfitting and bias, as in ridge regression. When dealing with non-linear problems, go-to models include polynomial regression (for example, used for trendline fitting in Microsoft Excel [59]), Logistic regression (often used in statistical classification) or even kernel regression, which introduces non-linearity by taking advantage of the kernel trick to implicitly map input variables to higher dimensional space. \\n A Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\\n A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.[60][61] Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.[62]\\n Usually, machine learning models require a lot of data in order for them to perform well. Usually, when training a machine learning model, one needs to collect a large, representative sample of data from a training set. Data from the training set can be as varied as a corpus of text, a collection of images, and data collected from individual users of a service. Overfitting is something to watch out for when training a machine learning model.\\n Federated learning is a new approach to training machine learning models that decentralizes the training process, allowing for users\\' privacy to be maintained by not needing to send their data to a centralized server. This also increases efficiency by decentralizing the training process to many devices. For example, Gboard uses federated machine learning to train search query prediction models on users\\' mobile phones without having to send individual searches back to Google.[63]\\n There are many applications for machine learning, including:\\n In 2006, the media-services provider Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1\\xa0million.[65] Shortly after the prize was awarded, Netflix realized that viewers\\' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly.[66] In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.[67] In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors\\' jobs would be lost in the next two decades to automated machine learning medical diagnostic software.[68] In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences among artists.[69] In 2019 Springer Nature published the first research book created using machine learning.[70]\\n Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results.[71][72][73] Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.[74]\\n In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision.[75] Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of investment.[76][77]\\n Machine learning approaches in particular can suffer from different data biases. A machine learning system trained on current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society.[78] Language models learned from data have been shown to contain human-like biases.[79][80] Machine learning systems used for criminal risk assessment have been found to be biased against black people.[81][82] In 2015, Google photos would often tag black people as gorillas,[83] and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorillas from the training data, and thus was not able to recognize real gorillas at all.[84] Similar issues with recognizing non-white people have been found in many other systems.[85] In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language.[86] Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.[87] Concern for fairness in machine learning, that is, reducing bias in machine learning and propelling its use for human good is increasingly expressed by artificial intelligence scientists, including Fei-Fei Li, who reminds engineers that \"There’s nothing artificial about AI...It’s inspired by people, it’s created by people, and—most importantly—it impacts people. It is a powerful tool we are only just beginning to understand, and that is a profound responsibility.”[88]\\n Classification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.[89]\\n In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model\\'s diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver Operating Characteristic (ROC) and ROC\\'s associated Area Under the Curve (AUC).[90]\\n Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.[91] For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.[92][93] Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.\\n Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.[94][95]\\n Other forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public\\'s interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm\\'s proprietary owners hold stakes. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these \"greed\" biases are addressed.[96]\\n Software suites containing a variety of machine learning algorithms include the following:\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N94d2y0-sgB",
        "colab_type": "text"
      },
      "source": [
        "As expected, the above result shows that the text is from the wikipedia page of Machine Learning. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-ydzmiu-Zc4",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER.3 Run the above function `get_wiki_data()` with some of your favorite wikipedia pages below and see if you get the appropriate result. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPpIew8elzD_",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SllZXQX6-0Q1",
        "colab_type": "code",
        "outputId": "2aaadc76-3324-4684-8c8b-e163421209be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "get_wiki_data(r\"https://en.wikipedia.org/wiki/supervised_learning\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Supervised learning is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.[1] It infers a function from labeled training data consisting of a set of training examples.[2]  In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal).  A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a \"reasonable\" way (see inductive bias).\\n The parallel task in human and animal psychology is often referred to as concept learning.\\n In order to solve a given problem of supervised learning, one has to perform the following steps:\\n A wide range of supervised learning algorithms are available, each with its strengths and weaknesses. There is no single learning algorithm that works best on all supervised learning problems (see the No free lunch theorem).\\n There are four major issues to consider in supervised learning:\\n A first issue is the tradeoff between bias and variance.[3]  Imagine that we have available several different, but equally good, training data sets.  A learning algorithm is biased for a particular input \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n if, when trained on each of these data sets, it is systematically incorrect when predicting the correct output for \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n.  A learning algorithm has high variance for a particular input \\n\\n\\n\\nx\\n\\n\\n{\\\\displaystyle x}\\n\\n if it predicts different output values when trained on different training sets.  The prediction error of a learned classifier is related to the sum of the bias and the variance of the learning algorithm.[4]  Generally, there is a tradeoff between bias and variance.  A learning algorithm with low bias must be \"flexible\" so that it can fit the data well.  But if the learning algorithm is too flexible, it will fit each training data set differently, and hence have high variance.  A key aspect of many supervised learning methods is that they are able to adjust this tradeoff between bias and variance (either automatically or by providing a bias/variance parameter that the user can adjust).\\n The second issue is the amount of training data available relative to the complexity of the \"true\" function (classifier or regression function).  If the true function is simple, then an \"inflexible\" learning algorithm with high bias and low variance will be able to learn it from a small amount of data.  But if the true function is highly complex (e.g., because it involves complex interactions among many different input features and behaves differently in different parts of the input space), then the function will only be able to learn from a very large amount of training data and using a \"flexible\" learning algorithm with low bias and high variance.\\n A third issue is the dimensionality of the input space.  If the input feature vectors have very high dimension, the learning problem can be difficult even if the true function only depends on a small number of those features.  This is because the many \"extra\" dimensions can confuse the learning algorithm and cause it to have high variance.  Hence, high input dimensional typically requires tuning the classifier to have low variance and high bias.  In practice, if the engineer can manually remove irrelevant features from the input data, this is likely to improve the accuracy of the learned function.  In addition, there are many algorithms for feature selection that seek to identify the relevant features and discard the irrelevant ones.  This is an instance of the more general strategy of dimensionality reduction, which seeks to map the input data into a lower-dimensional space prior to running the supervised learning algorithm.\\n A fourth issue is the degree of noise in the desired output values (the supervisory target variables).  If the desired output values are often incorrect (because of human error or sensor errors), then the learning algorithm should not attempt to find a function that exactly matches the training examples.  Attempting to fit the data too carefully leads to overfitting.  You can overfit even when there are no measurement errors (stochastic noise) if the function you are trying to learn is too complex for your learning model. In such a situation, the part of the target function that cannot be modeled \"corrupts\" your training data - this phenomenon has been called deterministic noise. When either type of noise is present, it is better to go with a higher bias, lower variance estimator.\\n In practice, there are several approaches to alleviate noise in the output values such as early stopping to prevent overfitting as well as detecting and removing the noisy training examples prior to training the supervised learning algorithm.  There are several algorithms that identify noisy training examples and removing the suspected noisy training examples prior to training has decreased generalization error with statistical significance.[5][6]\\n Other factors to consider when choosing and applying a learning algorithm include the following:\\n When considering a new application, the engineer can compare multiple learning algorithms and experimentally determine which one works best on the problem at hand (see cross validation).  Tuning the performance of a learning algorithm can be very time-consuming.  Given fixed resources, it is often better to spend more time collecting additional training data and more informative features than it is to spend extra time tuning the learning algorithms.\\n The most widely used learning algorithms are: \\n Given a set of \\n\\n\\n\\nN\\n\\n\\n{\\\\displaystyle N}\\n\\n training examples of the form \\n\\n\\n\\n{\\n(\\n\\nx\\n\\n1\\n\\n\\n,\\n\\ny\\n\\n1\\n\\n\\n)\\n,\\n.\\n.\\n.\\n,\\n(\\n\\nx\\n\\nN\\n\\n\\n,\\n\\n\\ny\\n\\nN\\n\\n\\n)\\n}\\n\\n\\n{\\\\displaystyle \\\\{(x_{1},y_{1}),...,(x_{N},\\\\;y_{N})\\\\}}\\n\\n such that \\n\\n\\n\\n\\nx\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle x_{i}}\\n\\n is the feature vector of the i-th example and \\n\\n\\n\\n\\ny\\n\\ni\\n\\n\\n\\n\\n{\\\\displaystyle y_{i}}\\n\\n is its label (i.e., class), a learning algorithm seeks a function \\n\\n\\n\\ng\\n:\\nX\\n→\\nY\\n\\n\\n{\\\\displaystyle g:X\\\\to Y}\\n\\n, where \\n\\n\\n\\nX\\n\\n\\n{\\\\displaystyle X}\\n\\n is the input space and\\n\\n\\n\\n\\nY\\n\\n\\n{\\\\displaystyle Y}\\n\\n is the output space.  The function \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n is an element of some space of possible functions \\n\\n\\n\\nG\\n\\n\\n{\\\\displaystyle G}\\n\\n, usually called the hypothesis space.  It is sometimes convenient to\\nrepresent \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n using a scoring function \\n\\n\\n\\nf\\n:\\nX\\n×\\nY\\n→\\n\\nR\\n\\n\\n\\n{\\\\displaystyle f:X\\\\times Y\\\\to \\\\mathbb {R} }\\n\\n such that \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n is defined as returning the \\n\\n\\n\\ny\\n\\n\\n{\\\\displaystyle y}\\n\\n value that gives the highest score: \\n\\n\\n\\ng\\n(\\nx\\n)\\n=\\n\\n\\n\\narg\\n\\u2061\\nmax\\n\\ny\\n\\n\\n\\nf\\n(\\nx\\n,\\ny\\n)\\n\\n\\n{\\\\displaystyle g(x)={\\\\underset {y}{\\\\arg \\\\max }}\\\\;f(x,y)}\\n\\n.  Let \\n\\n\\n\\nF\\n\\n\\n{\\\\displaystyle F}\\n\\n denote the space of scoring functions.\\n Although \\n\\n\\n\\nG\\n\\n\\n{\\\\displaystyle G}\\n\\n and \\n\\n\\n\\nF\\n\\n\\n{\\\\displaystyle F}\\n\\n can be any space of functions, many learning algorithms are probabilistic models where \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n takes the form of a conditional probability model \\n\\n\\n\\ng\\n(\\nx\\n)\\n=\\nP\\n(\\ny\\n\\n|\\n\\nx\\n)\\n\\n\\n{\\\\displaystyle g(x)=P(y|x)}\\n\\n, or \\n\\n\\n\\nf\\n\\n\\n{\\\\displaystyle f}\\n\\n takes the form of a joint probability model \\n\\n\\n\\nf\\n(\\nx\\n,\\ny\\n)\\n=\\nP\\n(\\nx\\n,\\ny\\n)\\n\\n\\n{\\\\displaystyle f(x,y)=P(x,y)}\\n\\n.  For example, naive Bayes and linear discriminant analysis are joint probability models, whereas logistic regression is a conditional probability model.\\n There are two basic approaches to choosing \\n\\n\\n\\nf\\n\\n\\n{\\\\displaystyle f}\\n\\n or \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n: empirical risk minimization and structural risk minimization.[7]  Empirical risk minimization seeks the function that best fits the training data.  Structural risk minimization includes a penalty function that controls the bias/variance tradeoff.\\n In both cases, it is assumed that the training set consists of a sample of independent and identically distributed pairs, \\n\\n\\n\\n(\\n\\nx\\n\\ni\\n\\n\\n,\\n\\n\\ny\\n\\ni\\n\\n\\n)\\n\\n\\n{\\\\displaystyle (x_{i},\\\\;y_{i})}\\n\\n.  In order to measure how well a function fits the training data, a loss function \\n\\n\\n\\nL\\n:\\nY\\n×\\nY\\n→\\n\\n\\nR\\n\\n\\n≥\\n0\\n\\n\\n\\n\\n{\\\\displaystyle L:Y\\\\times Y\\\\to \\\\mathbb {R} ^{\\\\geq 0}}\\n\\n is defined.  For training example \\n\\n\\n\\n(\\n\\nx\\n\\ni\\n\\n\\n,\\n\\n\\ny\\n\\ni\\n\\n\\n)\\n\\n\\n{\\\\displaystyle (x_{i},\\\\;y_{i})}\\n\\n, the loss of predicting the value \\n\\n\\n\\n\\n\\n\\ny\\n^\\n\\n\\n\\n\\n\\n{\\\\displaystyle {\\\\hat {y}}}\\n\\n is \\n\\n\\n\\nL\\n(\\n\\ny\\n\\ni\\n\\n\\n,\\n\\n\\n\\ny\\n^\\n\\n\\n\\n)\\n\\n\\n{\\\\displaystyle L(y_{i},{\\\\hat {y}})}\\n\\n.\\n The risk \\n\\n\\n\\nR\\n(\\ng\\n)\\n\\n\\n{\\\\displaystyle R(g)}\\n\\n of function \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n is defined as the expected loss of \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n.  This can be estimated from the training data as\\n In empirical risk minimization, the supervised learning algorithm seeks the function \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n that minimizes \\n\\n\\n\\nR\\n(\\ng\\n)\\n\\n\\n{\\\\displaystyle R(g)}\\n\\n.  Hence, a supervised learning algorithm can be constructed by applying an optimization algorithm to find \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n.\\n When \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n is a conditional probability distribution \\n\\n\\n\\nP\\n(\\ny\\n\\n|\\n\\nx\\n)\\n\\n\\n{\\\\displaystyle P(y|x)}\\n\\n and the loss function is the negative log likelihood: \\n\\n\\n\\nL\\n(\\ny\\n,\\n\\n\\n\\ny\\n^\\n\\n\\n\\n)\\n=\\n−\\nlog\\n\\u2061\\nP\\n(\\ny\\n\\n|\\n\\nx\\n)\\n\\n\\n{\\\\displaystyle L(y,{\\\\hat {y}})=-\\\\log P(y|x)}\\n\\n, then empirical risk minimization is equivalent to maximum likelihood estimation.\\n When \\n\\n\\n\\nG\\n\\n\\n{\\\\displaystyle G}\\n\\n contains many candidate functions or the training set is not sufficiently large, empirical risk minimization leads to high variance and poor generalization.  The learning algorithm is able\\nto memorize the training examples without generalizing well.  This is called overfitting.\\n Structural risk minimization seeks to prevent overfitting by incorporating a regularization penalty into the optimization.  The regularization penalty can be viewed as implementing a form of Occam\\'s razor that prefers simpler functions over more complex ones.\\n A wide variety of penalties have been employed that correspond to different definitions of complexity.  For example, consider the case where the function \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n is a linear function of the form\\n A popular regularization penalty is \\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\n\\nβ\\n\\nj\\n\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle \\\\sum _{j}\\\\beta _{j}^{2}}\\n\\n, which is the squared Euclidean norm of the weights, also known as the \\n\\n\\n\\n\\nL\\n\\n2\\n\\n\\n\\n\\n{\\\\displaystyle L_{2}}\\n\\n norm.  Other norms include the \\n\\n\\n\\n\\nL\\n\\n1\\n\\n\\n\\n\\n{\\\\displaystyle L_{1}}\\n\\n norm, \\n\\n\\n\\n\\n∑\\n\\nj\\n\\n\\n\\n|\\n\\n\\nβ\\n\\nj\\n\\n\\n\\n|\\n\\n\\n\\n{\\\\displaystyle \\\\sum _{j}|\\\\beta _{j}|}\\n\\n, and the \\n\\n\\n\\n\\nL\\n\\n0\\n\\n\\n\\n\\n{\\\\displaystyle L_{0}}\\n\\n norm, which is the number of non-zero  \\n\\n\\n\\n\\nβ\\n\\nj\\n\\n\\n\\n\\n{\\\\displaystyle \\\\beta _{j}}\\n\\ns.  The penalty will be denoted by \\n\\n\\n\\nC\\n(\\ng\\n)\\n\\n\\n{\\\\displaystyle C(g)}\\n\\n.\\n The supervised learning optimization problem is to find the function \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n that minimizes\\n The parameter \\n\\n\\n\\nλ\\n\\n\\n{\\\\displaystyle \\\\lambda }\\n\\n controls the bias-variance tradeoff.  When \\n\\n\\n\\nλ\\n=\\n0\\n\\n\\n{\\\\displaystyle \\\\lambda =0}\\n\\n, this gives empirical risk minimization with low bias and high variance.  When \\n\\n\\n\\nλ\\n\\n\\n{\\\\displaystyle \\\\lambda }\\n\\n is large, the learning algorithm will have high bias and low variance.  The value of \\n\\n\\n\\nλ\\n\\n\\n{\\\\displaystyle \\\\lambda }\\n\\n can be chosen empirically via cross validation.\\n The complexity penalty has a Bayesian interpretation as the negative log prior probability of \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n, \\n\\n\\n\\n−\\nlog\\n\\u2061\\nP\\n(\\ng\\n)\\n\\n\\n{\\\\displaystyle -\\\\log P(g)}\\n\\n, in which case \\n\\n\\n\\nJ\\n(\\ng\\n)\\n\\n\\n{\\\\displaystyle J(g)}\\n\\n is the posterior probabability of \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n.\\n The training methods described above are discriminative training methods, because they seek to find a function \\n\\n\\n\\ng\\n\\n\\n{\\\\displaystyle g}\\n\\n that discriminates well between the different output values (see discriminative model).  For the special case where \\n\\n\\n\\nf\\n(\\nx\\n,\\ny\\n)\\n=\\nP\\n(\\nx\\n,\\ny\\n)\\n\\n\\n{\\\\displaystyle f(x,y)=P(x,y)}\\n\\n is a joint probability distribution and the loss function is the negative log likelihood \\n\\n\\n\\n−\\n\\n∑\\n\\ni\\n\\n\\nlog\\n\\u2061\\nP\\n(\\n\\nx\\n\\ni\\n\\n\\n,\\n\\ny\\n\\ni\\n\\n\\n)\\n,\\n\\n\\n{\\\\displaystyle -\\\\sum _{i}\\\\log P(x_{i},y_{i}),}\\n\\n a risk minimization algorithm is said to perform generative training, because \\n\\n\\n\\nf\\n\\n\\n{\\\\displaystyle f}\\n\\n can be regarded as a generative model that explains how the data were generated.  Generative training algorithms are often simpler and more computationally efficient than discriminative training algorithms.  In some cases, the solution can be computed in closed form as in naive Bayes and linear discriminant analysis.\\n There are several ways in which the standard supervised learning problem can be generalized:\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nw5n-6Rf-pZv",
        "colab_type": "text"
      },
      "source": [
        "# Making sense of `get_wiki_data()` function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "19ZOW6KRO9aT"
      },
      "source": [
        "\n",
        "Now that we know the above function works, let us understand each component piece by piece. \n",
        "\n",
        "First line of the code states that `page = requests.get(url)`. This line gets the web page corresponding to the URL that we pass to the function `get_wiki_data(url)`. Then, by doing `page.text` we extract the text component of the response that we get. From that, `text` response, we make a soup object using the function `BeautifulSoup()`. Note here that the text that we extracted from the response is not the actual text content of the wikipedia page but rather it is the text version of the html extracted from the web page. \n",
        "\n",
        "The reason we pass the `HTML` to soup because this library makes it really easy to extract specific content using the `find_all` method. \n",
        "\n",
        "Let me show you how `find_all` works. \n",
        "\n",
        "Before that, let us learn to understand and read some basic `HTML` code. The `HTML` code of any wikipedia page can be `inspected` by hitting `CONTROL + SHIFT + I` on chrome and on safari `COMMAND + OPTION + I`. This inspect will open a new window on which you can view the `HTML` content of the corresponding web page that you are looking at. \n",
        "\n",
        "The following image shows the HTML Contents of Machine Learning Wikipedia Page. \n",
        "\n",
        "<p align = \"center\"><img src = \"https://raw.githubusercontent.com/datageekrj/ForHostingFiles/master/wikipediaInspect.PNG\" height = \"500\"></p>\n",
        "\n",
        "One Important thing that I want all of you to notice and pay attention on is thatr selected or highligted Machine Learning text on the right. Notice closely that it is between this strange looking symbols `<p>` and `<\\p>`. Now, these are called in `HTML` language paragraph tags. This is what we should be aware off. \n",
        "\n",
        "Now, next in the above `get_wiki_data` function, notice we do something like `soup.find_all(\"p\")`. With this code, what we are essentially doing is finding all such paragrapg tags and storing the result (which is the list of all paragrapg tags) in content. \n",
        "\n",
        "Now, that we have a list of all the paragraph tags, then we can loop over each one of the paragraph tag and get the text inside the corresponding tag and append the result in `data` list.\n",
        "\n",
        "Finally, we return the data after joining each text inside that list by seperating each text by space (`\" \"`)\n",
        "\n",
        "I hope how this function works is understood by everyone. \n",
        "\n",
        "I encourage everyone to try out this function with many other wikipedia page URLs. I want you play with the above function before moving to other bit of code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zu8z_JC2kfBO"
      },
      "source": [
        "## Pattern of URL in Wikipedia Pages.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e0awoW40sFN2"
      },
      "source": [
        "Consider a situation where you had some doubt in any topic and you go to Wikipedia to read about the corresponding topic. In that case, what you will most probably do is search for the topic name with wikipedia. Consider, the topic is `Machine Learning`, then you will probably search on google `Machine Learning Wikipedia` and this would yield number of results, the first of which will most probably the wikipedia page corresponding to your query. Now, when you go to that page, what you will essentially be going to is the URL `https://en.wikipedia.org/wiki/Machine_learning`. \n",
        "\n",
        "What if the topic was `Supervised Learning`, then the corresponding URL will be `https://en.wikipedia.org/wiki/Supervised_Learning`. I want Eveyone of you to notice the pattern in the URL of the page you are trying to search. \n",
        "\n",
        "Also, I want you all to go right now to google and search some about some of your favorite topics on wikipedia and i want you to notice the following two things:\n",
        "\n",
        "* If the topic is of only one word say x, then the corresponding URL will be `https://en.wikipedia.org/wiki/` and `x` concatenated or joined. Notice, that the first part which is `https://en.wikipedia.org/wiki/` will always be there. \n",
        "\n",
        "* If the topic is of more than one word, then each word will be joined by underscore (`_`) and a new word will be formed. For Example, in the `Machine Learning` example, the URL will be `https://en.wikipedia.org/wiki/Machine_Learning`\n",
        " \n",
        "So, the key takeaway is that each Wikipedia URL, will have the initial URL as `https://en.wikipedia.org/wiki/` and the other part will depend on whether your search query is of one word or it is of more than two words.\n",
        "\n",
        "Hence, we declare a variable called `wiki_initial` as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWoa-t15_HPf",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER.4 Declare a Variable called `wiki_initial` with the initial wikipedia URL that we expect? Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mdHq4Dnlve3",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LvSdit9PkOGN",
        "colab": {}
      },
      "source": [
        "wiki_intial = \"https://en.wikipedia.org/wiki/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4nEmzE3OsFOC"
      },
      "source": [
        "# Getting the Data\n",
        "\n",
        "Now, that we understand that to get the content of any wikipedia page we need to use the function `get_wiki_data(url)` which is going to take one argument as `url` which we just understood can be made easily by just appending the name of the topic to the `wiki_initial` variable name. Then, we will have the URL of the corresponding wikipedia page.\n",
        "\n",
        "So, to Make a Wikipedia Content Classification Model, we need to first decide:\n",
        "* What are the different topics we need?\n",
        "* How many different examples (no of wikipedia pages) we need for each topic?\n",
        "\n",
        "These two questions are very important as Machine Learning Model will be good only if we have sufficient number of examples (wikipedia articles) for each category. \n",
        "\n",
        "From the screenshot of the Final Application in the beginning, the following are the different topics that our Classification Model can classify a particular example to:\n",
        "* Celebrity\n",
        "* Statistics\n",
        "* Chemistry\n",
        "* Physics\n",
        "* Economics\n",
        "* Mathematics\n",
        "\n",
        "Okay, so, we have the answer of the First question and that was `What are the different topics we need?`. So, we have total six number of topics. \n",
        "\n",
        "Now, we need tom decide  `How many different examples (no of wikipedia pages) we need for each topic?`. This question is not easy to answer. It depends on person to person. If you want your model to have really good accuracy (You don't want your model to get wrong in prediction of the category), then in that case you will probably need really good number of examples for each category. \n",
        "\n",
        "There can be another person who just want something working really quick. In that case, you can probably have few examples for each of the category above. \n",
        "\n",
        "In this tutorial, we are going to be building an application with 300-400 examples but the code is easily extendable and I encourage you to add more examples. Doing this will be a piece of cake. You just need to find more topics and add that to the topic list that we are going to see in a little moment later. \n",
        "\n",
        "Now, note here we are going to have 200-300 examples, that means we are going to be using that function `get_wiki_data(url)` 200-300 times and it will take your data usage and time. So, depending upon the speed of your Internet Connection, you will probably have to wait for some time to get your data ready.\n",
        "\n",
        "Now, let us one by Collect data for each category. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WyTQT4etsFOD"
      },
      "source": [
        "# Collecting data for the Celebrity Group\n",
        "\n",
        "We have a function called `get_Wiki_data(url)` and with the help of this function, as discussed earlier, we can get the content of any wikipedia page with the corresponding URL. \n",
        "\n",
        "Also, we discussed for getting the URL of the corresponding web page, we just need to know about the name of the topic. \n",
        "\n",
        "So, if we have a list of names, we can loop through it and get the appropriate content. \n",
        "\n",
        "The first category is `Celebrity`. For getting some examples of celebrity category, we first need to have a list of names of celebrity.\n",
        "\n",
        "For this list, what i have done is, I have created a function called `get_list_ofimdb(url)` and what this function will do is get the of names of top 50 most popular hollywoord actors. \n",
        "\n",
        "We call this function on `https://www.imdb.com/list/ls053501318/` which is the corresponding link for the web page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QCoaa7dhsf-9",
        "colab": {}
      },
      "source": [
        "def get_list_of_imdb(url):\n",
        "  html = requests.get(url).text\n",
        "  content = BeautifulSoup(html)\n",
        "  req_list = []\n",
        "  for cont in content.find_all(\"div\", {\"class\":\"lister-item mode-detail\"}):\n",
        "    req_list.append(cont.find(\"h3\").find(\"a\").text)\n",
        "  return req_list\n",
        "actor_list_holly = get_list_of_imdb(\"https://www.imdb.com/list/ls053501318/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fSx9VNCBqGJ",
        "colab_type": "text"
      },
      "source": [
        "To get the list, first we make a request using request module and save the text response to `html` and then pass this to `BeautifulSoup` to make the soup and once that is done, we make an empty list and loop through the content one by one to get the appropriate content. \n",
        "\n",
        "You might wonder, why did we use `{\"class\":\"lister-item mode-detail\"}`, to demonstrate you this, I will inspect the corresponding imdb page and attach the image corresponding to this field to show the reason we added this class. Notice the yellow highlighted field below:\n",
        "\n",
        "<img src = \"https://raw.githubusercontent.com/datageekrj/ForHostingFiles/master/imdbinspect.PNG\">\n",
        "\n",
        "The class of the highlighted div corresponds to the `lister-item-mode-detail`. This is why we get that corresponding class. \n",
        "\n",
        "Using the `find_all` method on the soup content, we get a list of all the divs, and then we loop through those divs and find the `h3` tag and inside the h3 tag, notice that the `a` tag inside this `h3` tag and that tag contains the actor name. You need to look at the right of the above image (the yellow highlighted part). \n",
        "\n",
        "Finally, once we get the required list, we return that list. \n",
        "\n",
        "Now, notice that we call the function on the `URL` `https://www.imdb.com/list/ls053501318/`. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTEJFn-PIwat",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER.5 Check the length of the list `actor_list_holly`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s53tPEJAl9aR",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJHRQX4FI6-q",
        "colab_type": "code",
        "outputId": "0f05cf27-7492-4aee-8dbb-516a8ee46632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(actor_list_holly)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1uOQHmkJACV",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER.6 Check the name of the first actor from the list actor_list_holly. Try to answer yourself it yourself before looking at the answer\n",
        "\n",
        "Now, that we have the list of hollywoord actors, we can start to get the content corresponding to that. \n",
        "\n",
        "But first check the first actor name..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYkVMHR4mGHF",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jLV4r8DJqwU",
        "colab_type": "code",
        "outputId": "e7f067ef-fc3b-4229-cd07-7e8f5e567410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "actor_list_holly[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Johnny Depp\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbw1hodhJ9Qo",
        "colab_type": "text"
      },
      "source": [
        "# Cleaning the hollywood's actors names\n",
        " As you might have seen the name of the actors contain some un-wanting characters like `'\\n'`, `'\\s'`, `'^_'`. \n",
        "\n",
        " So, to get rid of that we loop through the name scraped from the imdb website and we replace that with an empty string `''`. \n",
        "\n",
        "Hence, we have a list of  names called `names_holly` which is a list of cleaned actors name. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXk_T7hNJ8At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names_holly = []\n",
        "for actor in actor_list_holly:\n",
        "  name = actor\n",
        "  name = re.sub(\"\\n\", \"\", name)\n",
        "  name = re.sub(\"\\s\", \"_\", name)\n",
        "  name = re.sub(\"^_\", \"\", name)\n",
        "  names_holly.append(name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo23iIRgLXsh",
        "colab_type": "text"
      },
      "source": [
        "# Scraping the content of the hollwood actor's wikipedia page\n",
        "\n",
        "Okay, now that we have the appropriate list of names, now we can use the `get_wiki_data(URL)` function and the `wiki_initial` variable that we declared earlier to get the text content of the wikipedia pages. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDMpNGTTLt71",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER. 7 Make a list `text_data_holly` which will have the scraped content from the wikipedia page using the function `get_wiki_data(url)` and `wiki_initial` variable. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewwtHOlhmLx6",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxzRvpUtLtSI",
        "colab_type": "code",
        "outputId": "bbdbd19e-9ed4-417b-939a-0d9cb7a993c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "text_data_holly = []\n",
        "for name in names_holly:\n",
        "  text_data_holly.append(get_wiki_data(wiki_intial+name))\n",
        "len(text_data_holly)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82b20mRNME1b",
        "colab_type": "text"
      },
      "source": [
        "# Making a Data Frame from the created `text_data_holly`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Lvc90vY9sFOQ",
        "outputId": "f944a27a-cbad-435b-f4dd-3868cf553a08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "hollywood = pd.DataFrame()\n",
        "hollywood[\"text\"] = text_data_holly\n",
        "hollywood[\"label\"] = [\"Celebrity\" for i in range(len(names_holly))]\n",
        "hollywood.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n John Christopher Depp II (born June 9, 1963...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n \\n Kevin Spacey Fowler KBE (born July 26, 1...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n \\n Denzel Hayes Washington Jr. (born Decemb...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text      label\n",
              "0  \\n John Christopher Depp II (born June 9, 1963...  Celebrity\n",
              "1  \\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...  Celebrity\n",
              "2  \\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...  Celebrity\n",
              "3  \\n \\n Kevin Spacey Fowler KBE (born July 26, 1...  Celebrity\n",
              "4  \\n \\n Denzel Hayes Washington Jr. (born Decemb...  Celebrity"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p7DIgJRzsFOc"
      },
      "source": [
        "## For other Categories (other than celebrity)\n",
        "\n",
        "Once we have the celebrity data from wikipedia, we now need to grab other categories of data and it turns out to be quite repetitive as we will have to first define a list of categories and then create data and further throw any bad data. So, there are three steps to it:\n",
        "\n",
        "* Make a List of Topics for each of the Catgories\n",
        "* Get wikipedia text data for each topic in that list\n",
        "* Throw out any bad data (which means throw out any wikipedia page which might now exist).\n",
        "\n",
        "These lists, I have prepared myself and the choice of topics are those that I could think of while creating them. Now, in case you want to have more examples (more than 200 - 300), you just have to come up with your own new topics and add those in their respective topic list. The remaining code will just work fine as the code has been generalised. Even if you want to grap 1000 wikipedia pages for each of the topics, then in that case also this code is going to work just perfectly fine considering you have the patience to sit while this scraps 1000s of wikipedia pages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Xwx8u2U2sFOd"
      },
      "source": [
        "## Creating Two Help Functions\n",
        "\n",
        "Now, since the way we approch the scraping part of remaining categories is quite repetitive for all the five categries (chemistry, physics, economice, mathematics, statistics), we will create two supporting functions `create_category_data(names, label)` and `throw_bad_data(names,label)`. These two functions will do the job for all the five remaining categories. \n",
        "\n",
        "Let us understand these two functions one by one. \n",
        "\n",
        "### `create_category_data(names, label)`\n",
        "\n",
        "This function will accept the list of topics as `names` and a corresponding group name as `label`. For example, I have got a list of topics of chemistry as `chem`, then i will call this function as `create_category_data(chem, \"Chemistry\")`. Here, `\"Chemistry\"` is the name of the label for the corresponding list of topics. Similay, for physics, we might call it like `create_category_data(phys, \"Physics\")` and so on.\n",
        "\n",
        "This function first loops through the list which is provided by `names` and gets the data for each wikipedia page by calling the function `get_wiki_data(url)` again and again. Here, URL is created by appending each topic name to the `wiki_initial`.\n",
        "\n",
        "Once, we get that list of all text, we create a data frame using pandas function `pd.DataFrame()`. This data frame will have two columns `text` and `label`. The `text` column will hold the list of texts that we just scraped for each of the topic and the `label` column will have the corresponding label which is going to be same for each text.\n",
        "\n",
        "Now, finally this function returns this data frame the we just created.\n",
        "\n",
        "### `throw_bad_data(names, label)`\n",
        "\n",
        "This function takes tha same arguments as the `create_category_data(names, label)`. \n",
        "\n",
        "First, it will call `create_category_data(names, label)` to get all the data (It will include both good and bad data). \n",
        "\n",
        "Now, you might be wondering that what do we mean by bad data? The list of topics that I createt, was essentially created by me without searching on internet. In this case, there might be a situation where a particular topic that I might have entered in this list does not have a corresponding wikipedia page. \n",
        "\n",
        "Now, what happens if we try to search for a word which does not have a corresponding wikipedia page. Then, in that case, this function `throw_bad_data(names, label)` will look for references of `may refer to:` text in all the collected text, and if it finds one, it will record its index and finally using pandas indexing, it removes those texts. \n",
        "\n",
        "The remaining data with all good wikipedia texts will be returned by this function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KfG0cIP3sFOe",
        "colab": {}
      },
      "source": [
        "def create_category_data(names,label):\n",
        "  text_data = [get_wiki_data(wiki_intial+i) for i in names]\n",
        "  data = pd.DataFrame()\n",
        "  data[\"text\"] = text_data\n",
        "  data[\"label\"] = [label for i in range(len(names))]\n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wHvB_8SfsFOn",
        "colab": {}
      },
      "source": [
        "def throw_bad_data(names,label):\n",
        "  data = create_category_data(names,label)\n",
        "  ind = []\n",
        "  for i,j in enumerate(data[\"text\"]):\n",
        "    if \"may refer to:\" in j:\n",
        "      ind.append(i)\n",
        "  irrelevant = data.index.isin(ind)\n",
        "  data = data[~irrelevant]\n",
        "  return(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "POhwE_VDsFOx"
      },
      "source": [
        "## Now, let us call this `throw_bad_data(names, label)` for each of the five categories and list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qc20R71OsFOy",
        "outputId": "a6a0a65e-3a19-42a3-f2e4-cd71636c5ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "chem = [\"compounds\", \"atoms\", \"molecules\", \"ions\", \"reaction\", \n",
        "\t\t\t \"chemist\", \"substances\", \"laboratory\", \"particles\",\n",
        "\t\t\t \"energy\", \"radioactive\", \"matter\", \"photon\", \"mass\",\n",
        "\t\t\t \"atomic_nucleus\", \"ionization\", \"metallic\", \"covalent\",\n",
        "\t\t\t \"periodic\", \"isotopes\", \"solids\", \"silica\", \"sulfur\", \n",
        "\t\t\t \"sodium\", \"carbon\", \"alloy\", \"avogadro\", \"liquid\", \"oxidation\",\n",
        "\t\t\t \"chloride\", \"plasma\", \"hydronium\", \"quanta\", \"phonons\", \"hydrogen\",\n",
        "\t\t\t \"sulfied\", \"kinetics\", \"hydroxide\", \"phosphate\", \"redox\", \"molar\",\n",
        "\t\t\t \"molarity\", \"helium\", \"krypton\"]\n",
        "chemistry = throw_bad_data(chem,\"Chemistry\")\n",
        "chemistry.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n An atom is the smallest constituent unit of...</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n A molecule is an electrically neutral group...</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n An ion (/ˈaɪɒn, -ən/)[1] is an atom or mole...</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Reaction may refer to a response to an action,...</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>A chemist (from Greek chēm (ía) alchemy; repla...</td>\n",
              "      <td>Chemistry</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text      label\n",
              "1  \\n An atom is the smallest constituent unit of...  Chemistry\n",
              "2  \\n A molecule is an electrically neutral group...  Chemistry\n",
              "3  \\n An ion (/ˈaɪɒn, -ən/)[1] is an atom or mole...  Chemistry\n",
              "4  Reaction may refer to a response to an action,...  Chemistry\n",
              "5  A chemist (from Greek chēm (ía) alchemy; repla...  Chemistry"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JHBsvyumYEB",
        "colab_type": "text"
      },
      "source": [
        "Let us define similar lists of other topics such as Physics, Maths, Statistics and Economics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77igkD-GOU5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phys = [\"energy\", \"force\", \"astronomy\", \"sun\", \"moon\", \"stars\",\n",
        "\t\t   \"planets\", \"optic\", \"gravitation\", \"gravity\", \"thermodynamics\", \n",
        "\t\t   \"electromagnetics\", \"theory_of_relativity\", \"photoelectric_effect\",\n",
        "\t\t   \"speed_of_light\", \"motion\", \"superconductivity\", \"higgs_boson\",\n",
        "\t\t   \"quantum\", \"quantum_mechanics\", \"supersymmetry\", \"neutrinos\", \"meissner_effect\",\n",
        "\t\t   \"magnetic_field\", \"fluxons\", \"electric_current\", \"geomagnetism\", \"inertia\",\n",
        "\t\t   \"newton's_laws_of_motion\", \"classical_mechanics\", \"kepler's_law\",\"planet\",\n",
        "\t\t   \"thermonuclear\", \"fusion\", \"orbit\", \"ellipse\"]\n",
        "\n",
        "maths = [\"arithmetic\", \"algebra\", \"geometry\", \"analysis\", \"set_theory\",\n",
        "\t\t \"number_theory\", \"natural_numbers\", \"complex_numbers\", \"infinity\",\n",
        "\t\t \"groups\", \"rings\", \"fields\", \"abstract_algebra\", \"linear_algenra\",\n",
        "\t\t \"vector_spaces\", \"analytic_geometry\", \"optimization\", \"tensor_calculus\",\n",
        "\t\t \"convex\", \"topology\", \"real_analysis\", \"differential_equations\", \"euclidean_geometry\",\n",
        "\t\t \"hypotenuse\", \"theorem\", \"volumes\", \"calculus\", \"integral\", \"differentiation\",\n",
        "\t\t \"limits\", \"riemann\", \"fourier_series\", \"prime_numbers\", \"measure\",\n",
        "\t\t \"infinite_series\", \"metric_space\", \"sine\", \"taylor_series\", \"lebesgue\",\n",
        "\t\t \"hilbert_spaces\", \"functional_analysis\", \"ordered_pair\"]\n",
        "\n",
        "stats = [\"probability\", \"surveys_sampling\", \"experiments\", \"inference\", \"observational_study\",\n",
        "\t     \"data_analysis\", \"mean\", \"variance\", \"dispersion\", \"descriptive_statistics\",\n",
        "\t     \"null_hypothesis\", \"missing_data\", \"probability_theory\", \"sampling\", \"estimator\",\n",
        "\t     \"statistic\", \"covariance\", \"mean_squared_error\", \"consistency\",\n",
        "\t     \"maximum_likelihood\", \"least_squares\", \"power_test\", \"random\", \"bayesian\",\n",
        "\t     \"frequentist\", \"significance\", \"critical_region\",\"correlation\", \"regression\", \n",
        "\t     \"logistic_regression\", \"factor_analysis\", \"cojoint_analysis\",\"parametric\",\n",
        "\t     \"time_series_analysis\", \"bootstrap\", \"markov_chain\"]\n",
        "\n",
        "eco = [\"production\", \"distribution_(economics)\", \"consumption_(economics)\", \"microeconomics\",\n",
        "\t\t\"macroeconomics\", \"demand\", \"supply\", \"national_income\", \"deficit\",\n",
        "\t\t\"resources\", \"market_equilibrium\", \"oligopoly\", \"monopoly\", \"duopoly\",\n",
        "\t\t\"monopsony\", \"investment_goods\", \"public_goods\", \"opportunity_cost\",\n",
        "\t\t\"factors_of_production\", \"pareto_efficiency\", \"production_possiblity_frontier\",\n",
        "\t\t\"scarcity\", \"constraints\", \"income_effect\", \"substitution_effect\",\n",
        "\t\t\"marginal_utility\", \"marginal_cost\", \"elasticity\", \"marginal_revenue\",\n",
        "\t\t\"perfectly_competitive\",\"game_theory\",\"economic_growth\", \"money\",\"balance_of_payments\",\n",
        "\t\t\"central_bank\", \"fiscal_policy\", \"monetary_policy\", \"monye_supply\",\n",
        "\t\t\"exchange_rates\", \"externalities\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJoTz-DLNqUq",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER. 8 Do the same with lists phys,maths,stats and eco and name the resulting data as `physics`,`mathematics`,`statistics` and `economics`. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNmNPJwzmV60",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_EA9CHeQJfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "physics = throw_bad_data(phys,\"Physics\")\n",
        "mathematics = create_category_data(maths,\"Mathematics\")\n",
        "statistics = create_category_data(stats,\"Statistics\")\n",
        "economics = create_category_data(eco,\"Economics\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DwVZBe5UsFPN"
      },
      "source": [
        "# Combining all of the above six data to one data frame\n",
        "\n",
        "Now, that we have data for all of the six categories, we can now concatenate every data frame to one data and call it `main_data` and finally we look at that data by using the `.head()` method on the resulting data frame.  \n",
        "\n",
        "For that we will call the `concat` method of the `pandas` and will pass all the data that we have created viz `hollywood`, `chemistry`, `physics`, `mathematics`, `statistics` and `economics`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NDXcxo63sFPP",
        "outputId": "21deb1a0-09d8-4e46-bfd6-b066966d01e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "main_data = pd.concat((hollywood,chemistry,physics,mathematics,statistics,economics),ignore_index=True)\n",
        "main_data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n John Christopher Depp II (born June 9, 1963...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n \\n Kevin Spacey Fowler KBE (born July 26, 1...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n \\n Denzel Hayes Washington Jr. (born Decemb...</td>\n",
              "      <td>Celebrity</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text      label\n",
              "0  \\n John Christopher Depp II (born June 9, 1963...  Celebrity\n",
              "1  \\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...  Celebrity\n",
              "2  \\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...  Celebrity\n",
              "3  \\n \\n Kevin Spacey Fowler KBE (born July 26, 1...  Celebrity\n",
              "4  \\n \\n Denzel Hayes Washington Jr. (born Decemb...  Celebrity"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n4DA1A9bsFPV"
      },
      "source": [
        "# STEP. 2 Preparing the data for Machine Learning Model\n",
        "\n",
        "Now, that we have a data named `main_data` ready for building machine learning model, we can start preparing data for building the model. \n",
        "\n",
        "## Why do we need to prepare data?\n",
        "\n",
        "The data that we have just scraped is in text and all the machine learning algorithms accept only numbers. So, we need a way to convert that text into some suitable numbers which makes sense. \n",
        "\n",
        "## Mapping the labels to numbers\n",
        "\n",
        "First, we will map each of the categories to some suitable numbers. The best choice would be number from 0-5 for each of the respective labels. To do that, we create a dictionary called `label_mapper` and then use the map function of the data frame, map each of the label. Then, we print the `main_data` to kind of see what we have got. Now, we have three columns, one extra for `label_number`.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NL6jeMpssFPY",
        "outputId": "00ae84f9-6089-48ed-a0e5-d4a64e9f5e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "label_mapper = {\"Celebrity\":0, \"Chemistry\":1, \"Physics\":2,\n",
        "               \"Mathematics\":3, \"Statistics\":4, \"Economics\":5}\n",
        "main_data[\"label_number\"] = main_data[\"label\"].map(label_mapper)\n",
        "main_data.head()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_number</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n John Christopher Depp II (born June 9, 1963...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n \\n Kevin Spacey Fowler KBE (born July 26, 1...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n \\n Denzel Hayes Washington Jr. (born Decemb...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text      label  label_number\n",
              "0  \\n John Christopher Depp II (born June 9, 1963...  Celebrity             0\n",
              "1  \\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...  Celebrity             0\n",
              "2  \\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...  Celebrity             0\n",
              "3  \\n \\n Kevin Spacey Fowler KBE (born July 26, 1...  Celebrity             0\n",
              "4  \\n \\n Denzel Hayes Washington Jr. (born Decemb...  Celebrity             0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tr6z-ZaTsFPf"
      },
      "source": [
        "## Text Cleaning and Stop Words Removal\n",
        "\n",
        "The text that we scraped from the wikipedia has lot of things that we are not intrested in. We are only intrested in the words (plain text). Further, we don't want those words which do not have much significance in predicting the category. One example of these kind of words is `stopwords`. Words such as `the,in,that etc` are said to be stopwords as they are often in every piece of english text and do not contain much discriminative power. \n",
        "\n",
        "So, for this step we perform the following two steps:\n",
        "* We remove the stopwords\n",
        "* We extract only words from the text or in other words, we get rid of numbers, or any unnecessay text by using regular expression\n",
        "\n",
        "Hence, we import a library called `nltk` which will give us access to most of the stopwords available out there. So, after importing `nltk`, we download the `stopwords` corpus and store the english stopwords to the `stop` variable. \n",
        "\n",
        "After storing these stopwords in a variable, we construct a function called `clean_text`, the objective of which is to get rid of unnecessary details. Only take words into consideration. \n",
        "\n",
        "### `clean_text(text)`\n",
        "\n",
        "This function will take `text` as input and it will first split that text into words. Now, after splitting the text into words, we will look for those words only which are not stopwords using the `stop` list that we got from the `nltk` library. \n",
        "\n",
        "After removing the stopwords, we then join all the text again and then using regular expression pattern, we split that new text on `[^A-Za-z]` which essentially means non-word character. Then, we join the resulting list on space and thus the new text will be free from any non word text. Finally, the resultant text will be returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_kDkf6QCtJKY",
        "colab": {}
      },
      "source": [
        "stop = ['i','me','my','myself','we','our','ours','ourselves','you',\"you're\",\"you've\",\"you'll\",\"you'd\",'your','yours','yourself','yourselves',\n",
        " 'he','him','his','himself','she',\"she's\",'her','hers','herself','it',\"it's\",'its','itself','they','them','their','theirs','themselves','what','which',\n",
        " 'who','whom','this','that',\"that'll\",'these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing','a',\n",
        " 'an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after',\n",
        " 'above','below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both',\n",
        " 'each','few','more','most','other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don',\"don't\",'should',\n",
        " \"should've\",'now','d','ll','m','o','re','ve','y','ain','aren',\"aren't\",'couldn',\"couldn't\",'didn',\"didn't\",'doesn',\"doesn't\",'hadn',\"hadn't\",'hasn',\"hasn't\",'haven',\n",
        " \"haven't\",'isn',\"isn't\",'ma','mightn',\"mightn't\",'mustn',\"mustn't\",'needn',\"needn't\",'shan',\"shan't\", 'shouldn',\"shouldn't\",'wasn',\"wasn't\",'weren',\"weren't\",'won',\"won't\",\n",
        " 'wouldn',\"wouldn't\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C6FjDv6XsFPg",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  tokens = text.split()\n",
        "  new = [word for word in tokens if word not in stop]\n",
        "  text = \" \".join(new)\n",
        "  text = \" \".join(re.split(r\"[^A-Za-z]\", text))\n",
        "  text = re.sub(\" +\", \" \", text)\n",
        "  return(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1qeblnu7sFPl"
      },
      "source": [
        "Finally, we loop through the `text` column of the datasets and call the `clean_text(text)` function for each text and append the cleaned text in `cleaned` list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ozLDLpKRK9p",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER.9 MAKE a list `cleaned` using the `text` column of the main data by calling the `clean_text(text)` function in a loop. Then, assign this cleaned list to a new column named `clean_text` of the `main_data` data frame. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCVGOt2Rmih7",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oYCe1M2AsFPm",
        "outputId": "4a038656-d8d5-4250-d323-38ef7779f9f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "cleaned = []\n",
        "for i in main_data.text:\n",
        "  cleaned.append(clean_text(i))\n",
        "main_data[\"clean_text\"] = cleaned\n",
        "main_data.head()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>label_number</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\\n John Christopher Depp II (born June 9, 1963...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "      <td>john christopher depp ii born june american ac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "      <td>alfredo james pacino p t i no italian pa t i n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>\\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "      <td>robert anthony de niro jr d n ro italian de ni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\\n \\n Kevin Spacey Fowler KBE (born July 26, 1...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "      <td>kevin spacey fowler kbe born july american act...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>\\n \\n Denzel Hayes Washington Jr. (born Decemb...</td>\n",
              "      <td>Celebrity</td>\n",
              "      <td>0</td>\n",
              "      <td>denzel hayes washington jr born december ameri...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         clean_text\n",
              "0  \\n John Christopher Depp II (born June 9, 1963...  ...  john christopher depp ii born june american ac...\n",
              "1  \\n \\n Alfredo James Pacino (/pəˈtʃiːnoʊ/; Ital...  ...  alfredo james pacino p t i no italian pa t i n...\n",
              "2  \\n \\n Robert Anthony De Niro Jr. (/də ˈnɪəroʊ/...  ...  robert anthony de niro jr d n ro italian de ni...\n",
              "3  \\n \\n Kevin Spacey Fowler KBE (born July 26, 1...  ...  kevin spacey fowler kbe born july american act...\n",
              "4  \\n \\n Denzel Hayes Washington Jr. (born Decemb...  ...  denzel hayes washington jr born december ameri...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sZS_ZoXfsFPw"
      },
      "source": [
        "# STEP.3 Building the Machine Learning Model\n",
        "\n",
        "So far we have scraped the data from wikipedia, cleaned the data and now it is time to build the model. As we know that we cannot "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFGq5Ld4RsMt",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER. 10 Extract the `clean_text` column to a new variable `X` and the `label_number` variable to new variable `y`. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2HIzHTMmlWp",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFbs4KgfRrjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = main_data.clean_text\n",
        "y = main_data.label_number"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SOgh2wRTEB3",
        "colab_type": "text"
      },
      "source": [
        "# Model Building\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsKKZ5ucTLA8",
        "colab_type": "text"
      },
      "source": [
        "Now, that we have two variables `X` and `y` as `text` and `label_number`, we can get started with the model building process.  \n",
        "\n",
        "There are three steps to the model building process:\n",
        "\n",
        "* We cannot just feed the text to the model, there has be some way of transforming that text into numbers by some suitable transformations. So, we use `CountVectorizer()` of `sklearn` module to convert the text into counts of words or also known as `bag-of-word` representation.\n",
        "* Once, we have `bag-of-word` representation, we can feed that to our algorithm which is `Multinomial Naive Bayes` algorithm but the count vectorizer is not free from faults. First, it gives unnecessary counts to some non-informative words. For example, if we have a certain word which appears in almost all of the text and that particular word will have most likely least predictive power and that word cannot differentiate between two sentences. Hence, we need something more powerful. Hence, in this step we use something which is known as tfidf transformer. Here, tfidf stands for term-frequency-inverse-document-frequency. Here, term-frequency is what its name represents. It is the number representing how many times a word appears. \n",
        "Next, we have inverse term frequency which gives us some idea about how important a word is with respect to the rest of the collection of documents. \n",
        "\n",
        "* Now, finally, we have Multinomial Naive Bayes algorithm which is implemented in the `MultinomialNB()` class of `sklean`.\n",
        "\n",
        "One of the nice things about sklean is that it provides us with the `Pipeline()` class which accepts all the preprocessors and alorithms in a list and gives us a nice way of training model in a pipeline. \n",
        "\n",
        "So, we pass each of the class in the pipeline in a list. Here, each element in a list is a tuple taking two elements. The first the name of the argument of the pipeline and the second is the actual class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tzZxe0ndsFP7",
        "colab": {}
      },
      "source": [
        "pipeline = Pipeline([\n",
        "    (\"count_vect\", CountVectorizer()),\n",
        "    (\"tfidf\", TfidfTransformer()),\n",
        "    (\"clf\", MultinomialNB())\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L_KPyrQV7S-",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER. 11 Call the fit method of the pipeline on the two variables `X` and `y`. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsZcSIaHmpen",
        "colab_type": "text"
      },
      "source": [
        "### Answer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZV7ph836sFQB",
        "outputId": "c3abc6c2-4330-4143-8927-2be55c823f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "pipeline.fit(X,y)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('count_vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5LN0u8UisFQG"
      },
      "source": [
        "# Creating Support Functions for Prediction From this model pipeline \n",
        "\n",
        "\n",
        "In particular, we are going to be creating two helping functions namely `from_url_to_predict_data(url)` and `get_prediction(url,model)`. Let us explain both of these functions one by one:\n",
        "\n",
        "### `from_url_to_predict_data(url)`\n",
        "\n",
        "This function is going to take a url and get the data from that url using the function that we created earlier called `get_wiki_data`. Once this data has been scraped, we clean that text using `clean_text(text)` function which you can find in the step 2 of preparing data. Then, finally we make a data using pandas will be created by passing that cleaned function to the `pd.Series` function.\n",
        "\n",
        "\n",
        "### `get_prediction(url,model)`\n",
        "\n",
        "This function will first get the data from the `from_url_to_predict_data(url)` function and then will predict using the `pipeline` model that we have trained on the main_data. Now, the prediction of the model will be a number and that needs to converted into a suitable label for the user to see. For this, we are going to loop through the `label_mapper` that we created earlier and the corresponding to the prediction result, the required label will be returned using this function. \n",
        "\n",
        "Both the function have been defined below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3VheDQkXsFQH",
        "colab": {}
      },
      "source": [
        "def from_url_to_predict_data(url):\n",
        "  text = get_wiki_data(url)\n",
        "  cleaned_text = clean_text(text)\n",
        "  data_for_prediction = pd.Series(cleaned_text)\n",
        "  return data_for_prediction\n",
        "\n",
        "def get_prediction(url,model):\n",
        "  req_data = from_url_to_predict_data(url)\n",
        "  pred = model.predict(req_data)\n",
        "  return [name for name,code in label_mapper.items() if code == pred][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m-3T1DtYsFQM"
      },
      "source": [
        "Now, that we have our model ready and also we have a way to get the prediction of the text, now it is the time to test our prediction on some of the wikipedia URLs. \n",
        "\n",
        "Remember, we didn't include `Machine Learning` wikipedia page in our training. Hence, let us first see what our model thinks about this wikipedia page."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qg91yvhYsFQN",
        "outputId": "01a3658e-4df3-4c38-ce97-d4a7a3166d83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "get_prediction(r\"https://en.wikipedia.org/wiki/Machine_learning\", pipeline)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Statistics'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KbAkuPsXXzAJ"
      },
      "source": [
        "It makes sense as out of the four topics, Machine Learning can either belong to statistics or mathematics. If we had also computer science as one of the categories , then we would have probably got the answer as Computer Science. \n",
        "\n",
        "Now, let us check for other wikipedia pages and see how our model that we trained performs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0KIdLkvWNFy",
        "colab_type": "text"
      },
      "source": [
        "# TASK FOR YOU\n",
        "\n",
        "## TASK NUMBER.12 Check the function `get_prediction(url, pipeline)` with some 3-4 urls of your wikipedia pages of your favorite topics. Try to answer yourself it yourself before looking at the answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCo-nKG7ms7D",
        "colab_type": "text"
      },
      "source": [
        "Answers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "56gGSvNwsFQR",
        "outputId": "27bda85f-9529-40b7-ba55-d60a336ccb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "[get_prediction(r\"https://en.wikipedia.org/wiki/Pareto\", pipeline),\n",
        " get_prediction(r\"https://en.wikipedia.org/wiki/computer_science\", pipeline),\n",
        " get_prediction(r\"https://en.wikipedia.org/wiki/hardy\", pipeline),\n",
        " get_prediction(r\"https://en.wikipedia.org/wiki/ramanujan\", pipeline),\n",
        " get_prediction(r\"https://en.wikipedia.org/wiki/confidence_interval\", pipeline),\n",
        " get_prediction(r\"https://en.wikipedia.org/wiki/factor_analysis\", pipeline),\n",
        " get_prediction(r\"https://en.wikipedia.org/wiki/big_bang_theory\", pipeline)]\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Economics',\n",
              " 'Mathematics',\n",
              " 'Mathematics',\n",
              " 'Celebrity',\n",
              " 'Statistics',\n",
              " 'Statistics',\n",
              " 'Physics']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0_WVV8ehY5SS"
      },
      "source": [
        "# STEP.4 Building the web application\n",
        "\n",
        "To build the web application, we are going to be using a framework in python called `flask`. It is one of the most lightweight frameworks out there. We have already imported the `Flask` class of the module, now let us initialize it and wrap it with `run_with_ngrok`. The reason that we are doing `run_with_ngrok` is because it will provide us with two links one for opening the web application locally and another for opening the web application on internet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XA5nfueJZa8e"
      },
      "source": [
        "Next step is to initialize our app. The `Flask` function accepts one argument called `__name__`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LSys87Eo2oyk",
        "colab": {}
      },
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpzwILi6XLw4",
        "colab_type": "text"
      },
      "source": [
        "# How does a any web application work?\n",
        "\n",
        "To learn about web application development, we need to know about the `HTML` and `CSS` which are two of the most important languages of web. \n",
        "\n",
        "We are going to talk about `HTML` and `CSS` in the next cell but right now I want to discuss how the flask app works.\n",
        "\n",
        "To make a flask app, there are following steps:\n",
        "\n",
        "* First make an `app` variable which is done in the above cell. \n",
        "* Then define the routes for app. For each route, we need to declare a function. In the later part of this tutorial, you will notice that we have made a function called `index()` before which we do something like `app.route(\"/\")`. This is a python function decorator. It means that it tells python that the function which proceeds after this decorator is a special function and in this case it is a route function which will be extecuted once user tries to access our application\n",
        "\n",
        "* Then, we have to decide what we want to show a user. The look and feel of the application is decided by `HTML` and `CSS`. \n",
        "\n",
        "* Them,once route function is set, we have to run the application using `app.run()`\n",
        "\n",
        "\n",
        "* A very simple example of a very simple flask app which will give you some idea.\n",
        "\n",
        "`from flask import Flask`\n",
        "\n",
        "`app = Flask(__name__)`\n",
        "\n",
        "`@app.route('/')`\n",
        "\n",
        "`def hello_world():`\n",
        "\n",
        "    `return 'Hello, World!'`\n",
        "\n",
        "`if __name__ == \"__main__\":`\n",
        "\n",
        "    `app.run()`\n",
        "\n",
        "In this veri simple and minimal application, once the server has started then the user will be greeted with a simple text \"Hello, World\". Now, we can put `HTML` code in the return statement but that would make the function very difficult to read, and this why we have clearly separated the string of the `HTML`. \n",
        "\n",
        "I hope this small discussion on how flask web app works makes sense.\n",
        "\n",
        "Now, let us understand a little bit about `HTML`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "plsFHpQGab5J"
      },
      "source": [
        "## Making sense of the following `html_style` variable. \n",
        "\n",
        "First of let us understand what the heck is this long string?\n",
        "\n",
        "If you are familiar with `HTML`, then the following string is just a piece of cake for you. But even if you are not, then also after reading this small discussion on `HTML` everything will start to make sense. \n",
        "\n",
        "### HTML - The language of Web\n",
        "\n",
        "HTML stands for `Hyper Text Markup Language` and it is the language of the web. With the help of HTML, we can define the structure of the web application that we want to build. \n",
        "\n",
        "`HTML` decides the structure of a web page. Whatever you see on the web page is structured because of `HTML`. Now, everything on our `wikipedia content classification application` is also structured using `HTML`.\n",
        "\n",
        "But Note here, HTML only handles the structure of the application while the look and feel of your application is handled by another technique called `CSS`. More on that later. \n",
        "\n",
        "To understand the following string, first of all understand that everything in `HTML` is made up of tags. \n",
        "\n",
        "What are tags? You might wonder..\n",
        "\n",
        "Tags in HTML are made up of opening and closing tags. Opening tags looks like `<tag_name>` and closing tags looks like `</tag_name>`. \n",
        "\n",
        "\n",
        "Let us tage the example of our application. It has a input box, a submit button, various texts. The first is the header of the application which is handled by header tags. There can be various different types of header tags. \n",
        "\n",
        "* `h1` header tag\n",
        "* `h2` header tag\n",
        "* `h3` header tag\n",
        "* `h4` header tag\n",
        "* `h4` header tag\n",
        "\n",
        "\n",
        "These all are used to place header for the HTML content. Each of them differs from each other because of the the size. \n",
        "\n",
        "Taking `h1` as an example, the opening tag would look like `<h1>` and the closing tag would look like `</h1>`. The content will be placed in between. \n",
        "\n",
        "Now, there are some types of tags which are self closing which means for those tags, there are no closing tags. `input` tag is one of the example self closing tag. \n",
        "\n",
        "### One special tag `<style> </style>`\n",
        "\n",
        "If you observe the following `html_style` text below, most of the content reside in between `<style>` and `</style>`. If you don't notice this immediately, I want all of you to stop right now and notice this right now. Then, you can move on.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghp5mvCQaY7U",
        "colab_type": "text"
      },
      "source": [
        "# Understanding HTML and CSS with example\n",
        "\n",
        "`HTML` and `CSS` are very simple to learn and master. I will give you a very simple example to explain a few concepts. \n",
        "\n",
        "`HTML` \n",
        "\n",
        "Html has two main tags `<head></head>` and `<body></body>`. Notice, there is no body tag in the following `html_style` variable. The reason is because the body is what gets displayed to users and that part is explained in later article called `Two States of Application`\n",
        "\n",
        "Then, what is the use of this `<head></head>` tag. It turns our that this head tag holds those things that user does not see but they have a major impact on any web page. \n",
        "\n",
        "\n",
        "One of the important things that this head tag holds is `<style></style>` tag. \n",
        "\n",
        "This tag holds all the color, size, display, shadow, radius of HTML elements etc. \n",
        "\n",
        "This style tag holds something which is known as CSS, `casacading style sheets`, it enables the web application to have the look that we want. \n",
        "\n",
        "In CSS, we have a concept of selectors and those selectors enables us to style individual selections. \n",
        "\n",
        "For example, I want to style the `<h1></h1>` tag, then I can select that tag in CSS as \n",
        "\n",
        "h1 {\n",
        "I can put all the different style property values to the h1 HTML tag.\n",
        "}\n",
        "\n",
        "In this similar manner, I can style all the HTML tags. \n",
        "\n",
        "If you want, you can also change some of the properties that are written below to see the effect of those on the final application. \n",
        "\n",
        "\n",
        "Now, you might be wondering do I have to memorise all these rules?\n",
        "\n",
        "The answer is `No`. The reason is that they are huge in numbers and all you need to do is type what style you are looking for in the google search bar and the corresponding style will come up as a first page on the internet. \n",
        "\n",
        "You need to learn these `HTML tags and CSS rules` only if you are really intrested about getting into web development. \n",
        "\n",
        "But on the other hand, if you are just looking for creating machine learning web applications, then you don't have to learn these rules of HTML and CSS but if you could, that would be really helpful and you will be able to create wonderful web applications. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "omKNgS8T2ovK",
        "colab": {}
      },
      "source": [
        "html_style = '''\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "    <meta charset=\"UTF-8\">\n",
        "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "    <title>New Style</title>\n",
        "    <style>\n",
        "        body,html,*{\n",
        "            margin:0px;\n",
        "            padding: 0px;\n",
        "        }\n",
        "        .nesto-background{\n",
        "            background: url(https://dtsvkkjw40x57.cloudfront.net/1350xnull/8160/uploads/2ab85f0f-bc62-417f-8a33-a064535a6ebd.jpg);\n",
        "            opacity: 0.9;\n",
        "            height: 300px;\n",
        "            width: 100%;\n",
        "            margin: 0px 0px 10px 0px;\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "        }\n",
        "        .nesto-background div{\n",
        "            align-self: flex-end;\n",
        "        }\n",
        "\n",
        "        h1 {\n",
        "            border: 0px solid #F49200;\n",
        "            background-color: #F49200;\n",
        "            border-radius: 5px;\n",
        "        }\n",
        "        .remaining{\n",
        "            display: flex;\n",
        "            flex-direction: row;\n",
        "        }\n",
        "        .form{\n",
        "            width: 60%;\n",
        "            align-self: flex-start;\n",
        "            margin-top: 100px;\n",
        "        }\n",
        "        .info{\n",
        "            align-self: flex-end;\n",
        "            display: flex;\n",
        "            width:40%;\n",
        "            flex-direction: column;\n",
        "            justify-content: center;\n",
        "            align-items: center;\n",
        "            margin-top: 100px;\n",
        "            margin-left:15px;\n",
        "        }\n",
        "        \n",
        "        form{\n",
        "            display: flex;\n",
        "            flex-direction: column;\n",
        "        }\n",
        "        input[name=url]{\n",
        "            width: 200px;\n",
        "            height: 25px;\n",
        "            margin-top: 20px;\n",
        "            align-self: center;\n",
        "        }\n",
        "\n",
        "        input[name=submit]{\n",
        "            width: 100px;\n",
        "            height: 30px;\n",
        "            outline: none;\n",
        "            border: none;\n",
        "            background-color: #F49200;\n",
        "            color: white;\n",
        "            margin-top: 10px;\n",
        "            align-self: center;\n",
        "        }\n",
        "\n",
        "        h4{\n",
        "            font-size: 20px;\n",
        "            color: #F49200;\n",
        "        }\n",
        "        h3{\n",
        "            color: red;\n",
        "            margin-top: 10px;\n",
        "        }\n",
        "        ul{\n",
        "            margin-top: 10px;\n",
        "        }\n",
        "        li{\n",
        "            margin-left: 30px;\n",
        "        }\n",
        "        h2{\n",
        "            color: #F49200;\n",
        "        }\n",
        "    </style>\n",
        "</head>\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Txj5e7rLifiH"
      },
      "source": [
        "## Two states of our application - Posted or not Posted\n",
        "\n",
        "The final application will be having two different states. One, where the user of the application will be shown the input box and some information about the application. The another state is where the user will submit the `URL` and tries to get the prediction that `URL`. \n",
        "\n",
        "So, for each of these states, I have different `HTML` string. One which is shown when the User visits the application `html_string_not_posted` and one where the user posts some URL and is expecting some return of prediction and for that notice we have created a function called `get_html_posted_string(data)` which accepts the data argument and that data is nothing but the prediction of the `URL` posted by user.\n",
        "\n",
        "Notice one thing, we add html_style to the beginning in both the states (`html_string_not_posted` and in the function `get_html_posted_string(data)`). It is because the application style (which is color, font size, and all other styles are consistent for both the states of the application). \n",
        "\n",
        "Hence, although the user might be looking at the welcome screen of the application or he might be looking at the screen where the result is displayed after he submitted his `URL`, for both of these states, the style should be same and that style we declared earlier as `html_style`.\n",
        "\n",
        "Now, remember the screen which is displayed once the user opens the app?\n",
        "\n",
        "This is the state when the `input` is shown. In that page, there were lot of things. Like text `Welcome to the Wkipedia Content Classification Application` and other texts. Also, we had list of categories informing users about the six categroies. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JgRP0qzp2zuZ",
        "colab": {}
      },
      "source": [
        "html_string_not_posted = html_style + '''\n",
        "<body>\n",
        "    <div class=\"nesto-background\">\n",
        "        <h1> Welcome to the Wikipedia Content Classification Application</h1>\n",
        "        <div> <img src = \"https://raw.githubusercontent.com/datageekrj/ForHostingFiles/master/NestoTV_1280x720%20(1).png\" width = \"200\" height = \"80\" style = \"align-self:center;\"></div>\n",
        "    </div>\n",
        "    <div class=\"remaining\">\n",
        "        <div class=\"form\">\n",
        "            <h4>To use this application, Just Enter the URL of the wikipedia Page that you want to classify and hit submit..</h4>\n",
        "            <form method=\"POST\">\n",
        "                <input type=\"text\" name=\"url\" class=\"url\" placeholder=\"Enter URL\">\n",
        "                <input type=\"submit\" name=\"submit\" class=\"submit\">\n",
        "            </form>\n",
        "        </div>\n",
        "        <div class=\"info categories\">\n",
        "            <h2>Categories Included while Training the Naive Bayes Algorithm: </h2>\n",
        "                <ul>\n",
        "                    <li>Celebrity</li>\n",
        "                    <li>Chemistry</li>\n",
        "                    <li>Physics</li>\n",
        "                    <li>Statistics</li>\n",
        "                    <li>Mathematics</li>\n",
        "                    <li>Economics</li>\n",
        "                <h3> Data Source: Data Was Scraped Using Wikipedia by Python using BeautifulSoup</h3>\n",
        "                </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVWvyT1YNJUM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Whereas on the other hand, once the user is displayed the prediction, there are only two things. \n",
        "\n",
        "* One is the `h1` tag which displays tha prediction and \n",
        "* another is a `a` tag for going back to the first state of the application. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SLY_9pb-2-W_",
        "colab": {}
      },
      "source": [
        "def get_html_posted_string(data):\n",
        "  required_string = html_style + '''\n",
        "<body>\n",
        "    <div class=\"nesto-background\">\n",
        "        <h1> Welcome to the Wikipedia Content Classification Application</h1>\n",
        "        <div> <img src = \"https://raw.githubusercontent.com/datageekrj/ForHostingFiles/master/NestoTV_1280x720%20(1).png\" width = \"200\" height = \"80\" style = \"align-self:center;\"></div>\n",
        "    </div>\n",
        "    <div class=\"remaining\">\n",
        "        <div style = \"display:flex;flex-direction:column;align-items:center;justify-content:center;\">\n",
        "            <h1>Your URL that you have entered belongs to: <span style=\"color: #707070;\">''' + data + '''\n",
        "</span></h1><br>\n",
        "        <a href=\"/\">Go Back</a>\n",
        "        </div>\n",
        "        <div class=\"info categories\">\n",
        "            <h2>Categories Included while Training the Naive Bayes Algorithm: </h2>\n",
        "                <ul>\n",
        "                    <li>Celebrity</li>\n",
        "                    <li>Chemistry</li>\n",
        "                    <li>Physics</li>\n",
        "                    <li>Statistics</li>\n",
        "                    <li>Mathematics</li>\n",
        "                    <li>Economics</li>\n",
        "                <h3> Data Source: Data Was Scraped Using Wikipedia by Python using BeautifulSoup</h3>\n",
        "                </ul>\n",
        "        </div>\n",
        "    </div>\n",
        "</body>\n",
        "</html>'''\n",
        "  return required_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tkq5biI3mmI5"
      },
      "source": [
        "## Creating the 'route' for our web application\n",
        "\n",
        "Now, that we have defined the necessry html structure, we need to create the route of the web application. \n",
        "\n",
        "### What is route?\n",
        "\n",
        "Route is just a fancy word for saying `URL`. Now, the `URL` which the user is going to enter in the input box is different from this URL. \n",
        "\n",
        "A web application is just like any other website you visit online. Now, when you visit any website you enter some `URL` and that URL is what we are talking about here.\n",
        "\n",
        "So, since we want our web application to run on our own system. We don't want to share it with world. So, we have to create some way to open the web application on our local system. \n",
        "\n",
        "Hence, we create route for this purpose. \n",
        "\n",
        "So, before the function `index()`, we have a statement which starts with `@` which has a special name in python `decoraters`. \n",
        "\n",
        "Inside this route we say that the URL is `'/'` and we will accept two types of methods `GET` and `POST`. \n",
        "\n",
        "### Now, let us understand what do we mean by methods?\n",
        "\n",
        "Remember, when we talked about the states of the application. There were two states:\n",
        "\n",
        "* Where you are welcomed to enter the `URL` == `GET` request\n",
        "* Once the user enters the `URL`, he will be displayed the prediction == `POST` request\n",
        "\n",
        "Also, if you remember, we had two different types of string for both of the different types methods. For `GET` requests, we had `html_string_not_posted` while for the `POST` request we had a function which will take the `prediction` and give us the corresponding `HTML` representation. \n",
        "\n",
        "Now, inside the `index()` function, we then test whether the type is 'POST' or it is 'GET'. \n",
        "\n",
        "\n",
        "If the method is not `POST`, in the else block, we just return the `html_string_not_posted` while in the post, we first get the `url` that the user has entered using the `request.form.get` method. Once, the `URL` has been obtained, we call the `get_prediction()` function with the `url` and the `pipeline` that we trained using the scraped data from the wikipedia. \n",
        "\n",
        "Once, we get the prediction from the function, we store the result in `prediction`and then the function for second state of the application will be called and that is `get_html_posted_string` and the resulting `HTML` representation will be returned and consequently returned.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XbxwAZGO3Q81",
        "colab": {}
      },
      "source": [
        "@app.route('/', methods = [\"GET\", \"POST\"])\n",
        "def index():\n",
        "    if request.method == \"POST\":\n",
        "        url = request.form.get(\"url\")\n",
        "        prediction = get_prediction(url, pipeline)\n",
        "        required_posted_string = get_html_posted_string(prediction)\n",
        "        return required_posted_string\n",
        "    else:\n",
        "        return html_string_not_posted"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_MCHJ1pttH6p"
      },
      "source": [
        "# Running the application\n",
        "\n",
        "Finally, once we have set up the `HTML` string and the route, it is time to run the application by calling the method `run()` on the app variable that we declared. It is going to start the `Flask Server` and it is going to point you to a local url [http://127.0.0.1:5000/](http://127.0.0.1:5000/). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CZ3htCU13Uvu",
        "outputId": "8afc8a51-b4db-466d-e361-bfeab17730b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  app.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://68b45824.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [01/May/2020 09:48:18] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [01/May/2020 09:48:20] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWZKZw40nFIw",
        "colab_type": "text"
      },
      "source": [
        "After running the above piece of code, the server will be started and it will point you to two URLs. First, a http://127.0.0.1:5000/ which is a local host URL and it will only work if you are running the web application on your own machine . \n",
        "\n",
        "If you are running on the Cloud, then the that case use the URL with `ngrok` in it and it will take you to a new page where you can play with the web application"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtlmKfKfnmDM",
        "colab_type": "text"
      },
      "source": [
        "**Hence, this completes our discussion on creating a wikipedia classification web application using Flask and Wikipedia We Scraping Using Beautiful Soup and requests. This newly acquired knowledge, you can utilise in many areas such as building some other types of web application. You just have to follow the same thought process.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUFciozbnlNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}